\section{Introduction}\label{sec:introduction}

\subsection{Background}

In the past years, DL models have been claimed to reach human parity on a range of tasks which were deemed challenging only a few years ago. For instance, machine translation is deemed on par with human translators on popular language pairs \citep{toral_attaining_2018}, scientific DL models are more accurate than explicit hand-crafted ones in a growing range of fields \citep{ravuri_skilful_2021}, while reinforcement learning agents based on DL models have outperformed professionals in multiple video games \citep{schrittwieser_mastering_2020} and industrial control applications \citep{degrave_magnetic_2022}.

The resounding success of recent DL work has in large part been attributed to the newly-gained ability of DL models to automatically extract relevant features from input data \citep{lecun_deep_2015}, rather than making use of hand-crafted features. This has proven effective in advancing the state-of-the-art on numerous DL tasks \citep{radford_learning_2021} \citep{radford_improving_nodate}. Concretely, the automatically-derived features are represented through the specific activation patterns of hidden layers as information propagates through the DL model \citep{lecun_deep_2015}. Besides early layers being able to already extract surface input features, a recurring finding has been the fact that input representations become increasingly abstract with each sequential layer, before collapsing again to the concrete particularities of the output towards the final layers \citep{tenney_bert_2019}. When such representations are continuous and dense, they are referred to as embeddings -- high-dimensional vectors subjected to sequential transformations specified through the model's parameters.

In the context of our increased reliance on DL models on a societal level, the field of explainable AI (XAI) investigates methods for interpreting the inner workings of such models, which otherwise lack clear internal rules due to being largely trained on raw data \citep{arrieta_explainable_2019}. Techniques of this kind help researchers debug such DL systems, ensuring their safe use in practice (e.g. avoids toxic cultural biases, is aligned to human values). A pervasive trade-off in XAI, however, is the conflict between formulating explanations which (1) accurately reflect the actual processing performed by DL models (i.e. functionally-grounded), yet (2) are highly comprehensible and intelligible for humans (i.e. human-grounded) \citep{madsen_post-hoc_2021}. This resembles a constant conflict faced in machine translation (MT), where models should (1) adequately preserve input meaning (i.e. adequacy), while (2) producing a coherent translation in itself (i.e. fluency) \citep{koehn_neural_2017}. Framing explainable AI as a translation task from machine to human representations has proven a useful lens for understanding the obstacles faced by the current work, as the adequacy-fluency trade-off in MT is intuitive.

Given the central role of input representations in recent DL models, a large body of advances in XAI has focused on distilling high-dimensional embeddings into a form which is interpretable by humans \citep{madsen_post-hoc_2021}. As the embeddings themselves arguably lack meaning when not grounded in input or output data, the vast majority of such work has specifically attempted to highlight how a certain DL model relates inputs and outputs by means of embeddings.

For instance, prior work has highlighted toxic gender biases in word embeddings (e.g. "woman" being represented as closer in meaning to "nurse" than "programmer" by a DL model), which promptly led to debiasing techniques being developed by the NLP community \citep{bolukbasi_man_2016}. Moreover, methods have been developed to construct ontologies from embeddings by means of hierarchical clustering, in order to better understand how the underlying DL model groups concepts together \citep{liu_interpretation_2018}. As another line of research, behavioral and structural probes have been employed to locate the "site" of various computations (e.g. part-of-speech tagging in NLP), by means of relating embeddings from different layers to cruder external representations (e.g. part-of-speech) \citep{tenney_bert_2019}. Additionally, methods have been suggested to explicitly represent abstraction relations using embedding "columns", hinting at future DL models which are partially explainable themselves, even before making use of post-hoc XAI tools \citep{hinton_how_2021}.

However, even when interpretability techniques focus on directly relating inputs to outputs, embeddings are often involved as mediators \citep{danilevsky_survey_nodate}. As an example of tracing specific input influences on the output, feature explanations highlight which particular aspects of the input data have been most influential in yielding the output \citep{jain_attention_nodate}. Alternatively, techniques based on counterfactuals and adversarial examples aim to find marginally different inputs which cause massive changes in output \citep{madsen_post-hoc_2021}. As a particularly ergonomic family of explanations, methods have also been developed to incentivize models to directly explain themselves in natural language \citep{madsen_post-hoc_2021}. Finally, another effective technique is based on extracting knowledge graphs by inferring entity-relationship-entity triples directly via (masked) language modeling \citep{wang_language_2020}.

In this context, we extend the existing toolkit of interpretability techniques by introducing a novel approach termed Nested State Clouds (NSC). This technique can be used to distill highly-comprehensible knowledge graphs directly from sets of high-dimensional embeddings, with no modality constraints. In other words, given a DL model and an auxiliary dataset, NSC appears to be a promising candidate technique for automatically organizing concepts in an abstraction hierarchy which reflects the model's internalized knowledge. An important benefit of NSC is its modality-agnostic design (i.e. appears applicable to DL models operating with various non-text modalities), in stark contrast to methods which only focus on distilling knowledge graphs from text \citep{wang_language_2020}. By automatically analyzing the way state clouds of contextual embeddings are positioned relative to each other in latent space, NSC appears capable of distilling high-dimensional representations which have been abstracted away from particular modalities. Through the present work, we investigate the question of whether the modality-agnostic spatial layout of high-dimensional embeddings can be meaningfully interpreted so as to yield a relevant knowledge graph.

However, in the context of this paper, we limit ourselves to an initial investigation of a pretrained masked \textit{language} model. Given this, we leave the applicability of NSC to arbitrary classification models (e.g. ViT \citep{dosovitskiy_image_2021}) for future work. We speculate on the tractability of generalizing NSC in the discussion by hinting at the possibility of making use of state clouds obtained from individual class member embeddings, rather than token embeddings.

Attempting to place NSC in the existing landscape of interpretability techniques, we note that our approach can generate global explanations (i.e. which attempt to describe the model's processing across multiple inferences) which are provided post-hoc (i.e. after training the model) \citep{danilevsky_survey_nodate}. This is in contrast to those XAI techniques which yield local explanations (i.e. describing the way a particular inference unfolds across layers) and those which are provided during the actual training of inherently interpretable models.

\subsection{NSC Overview}

Before introducing the core ideas behind NSC, we specify in more depth several terms which will be made heavy use of. By \textit{symbol}, we refer to the explicit way in which a concept is represented in text (e.g. the string "fruit"). Interestingly enough, our discussion illustrates plainly how the same symbol can refer to multiple concepts. By \textit{exemplar}, we refer to a specific instance of a symbol in a certain context (e.g. "fruit" in "It's healthy to eat \textbf{fruit}s regularly."). Importantly, each exemplar is a case of a symbol assuming specific semantics which are unique to its context (e.g. "fruit" could refer to an apple or a banana, depending on context). In practice, those specific semantics are represented numerically by means of contextual embeddings \citep{devlin_bert_nodate}. For the purposes of this work, we generally assume an exemplar-based view of concepts (e.g. defining the concept of fruit by means of the finite totality of its exemplars), as opposed to a prototype-based one (e.g. defining the concept of fruit based on one general idealized prototype).

NSC works by first generating a state cloud of contextual embeddings for each given symbol, representing the semantics of its exemplars (subfigure B of Fig. \ref{fig:exemplars-conceptors}). A state cloud is simply a set of such high-dimensional embeddings, which inevitably comes to exhibit a certain shape and directionality when regarded as a whole. This generation process is based on simply using the investigated model in inference mode together with the auxiliary corpus. For instance, the symbol "fruit" can refer to a host of different objects, depending on the context of use -- variation which is captured by distinct contextual embeddings. Following this initial step, NSC will have formed a collection of one state cloud of contextual embeddings per symbol. Each such state cloud will represent the distribution of semantics assumed by the various exemplars in their respective contexts.

Next, NSC compactly represents the overarching shape of each resulting state cloud as a \textit{conceptor} identified with a high-dimensional ellipsoid, instead of a large set of contextual embeddings \citep{jaeger_controlling_2017} (subfigure C of Fig. \ref{fig:exemplars-conceptors}). We find that this representation often results in orders-of-magnitude lower memory footprint compared to the naive approach of storing all individual embeddings. While the ellipsoids are conceptually similar to the ones obtained using principal component analysis (PCA), we opted for using conceptors as compact high-dimensional objects due to existing literature investigating meaningful ways of relating them to each other \citep{jaeger_controlling_2017}. Specifically, an inherent \textit{abstraction ordering} has been previously defined for pairs of conceptors (subfigure B of Fig. \ref{fig:conceptors-graph}). This refers to a means of comparing two such objects in terms of their level of abstraction, a process which we are exploiting in the present paper in an attempt to generate a knowledge graph. Loosely speaking, a conceptor which spatially engulfs another can be said to be more abstract, as the former encompasses a broader region of semantic space than the latter. We refer the reader to the relevant methods subsections for details on conceptors and their abstraction ordering.

After generating one state cloud per symbol, representing them as conceptor objects, and conducting pairwise comparisons of abstraction, an optimization algorithm is employed to generate the final output of NSC: a knowledge graph (subfigure C of Fig. \ref{fig:conceptors-graph}). Concretely, the algorithm based on simulated annealing is iteratively refining a directed graph which aims to accurately represent the estimated relations of abstraction. In contrast to simply constructing a directed graph by adding a new arc for each positive abstraction relation, an optimization algorithm allows us to better deal with noise. Additionally, the optimization framing enables us to specify additional "nice-to-have" properties of the desired output graph \citep{madsen_post-hoc_2021}. For instance, we penalize high numbers of arcs, parents per node, and children per node, in an attempt to keep the output explanations sparse and highly legible. In this, the objective function of the optimization algorithm essentially provides a "slider" between functionally-grounded and human-grounded explanations.

Our contributions are as follows:

\begin{itemize}
    \item We provide qualitative evidence highlighting the connection between the spatial layout of nested state clouds and the abstraction relation of the concepts they represent.
    \item We formulate an novel algorithm for flexibly distilling a set of high-dimensional state clouds into a compact directed graph which depicts relations of abstraction.
    \item We draw evidence-based observations on the way individual symbols relate to concepts in contextual embeddings.
\end{itemize}

In the first half of Section \ref{sec:methods}, we describe the model we are later applying NSC \textit{to}. In the second half, we describe in more depth the individual stages of the NSC algorithm. In Section \ref{sec:results}, we investigate the preliminary results of the novel technique. In Section \ref{sec:discussion}, we explore potential issues of the technique and highlight opportunities for future work.