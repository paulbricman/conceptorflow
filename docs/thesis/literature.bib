
@article{madsen_post-hoc_2021,
  title      = {Post-hoc {Interpretability} for {Neural} {NLP}: {A} {Survey}},
  shorttitle = {Post-hoc {Interpretability} for {Neural} {NLP}},
  url        = {http://arxiv.org/abs/2108.04840},
  abstract   = {Natural Language Processing (NLP) models have become increasingly more complex and widespread. With recent developments in neural networks, a growing concern is whether it is responsible to use these models. Concerns such as safety and ethics can be partially addressed by providing explanations. Furthermore, when models do fail, providing explanations is paramount for accountability purposes. To this end, interpretability serves to provide these explanations in terms that are understandable to humans. Central to what is understandable is how explanations are communicated. Therefore, this survey provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth. Furthermore, the survey focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic. A common concern for this class of methods is whether they accurately reﬂect the model. Hence, how these post-hoc methods are evaluated is discussed throughout the paper.},
  language   = {en},
  urldate    = {2022-02-03},
  journal    = {arXiv:2108.04840 [cs]},
  author     = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  month      = aug,
  year       = {2021},
  note       = {arXiv: 2108.04840},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  file       = {Madsen et al. - 2021 - Post-hoc Interpretability for Neural NLP A Survey.pdf:/home/paulbricman/Zotero/storage/TB8WUPHD/Madsen et al. - 2021 - Post-hoc Interpretability for Neural NLP A Survey.pdf:application/pdf}
}

@article{olah_building_2018,
  title    = {The {Building} {Blocks} of {Interpretability}},
  volume   = {3},
  issn     = {2476-0757},
  url      = {https://distill.pub/2018/building-blocks},
  doi      = {10.23915/distill.00010},
  abstract = {Interpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them -- and the rich structure of this combinatorial space.},
  language = {en},
  number   = {3},
  urldate  = {2022-02-12},
  journal  = {Distill},
  author   = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  month    = mar,
  year     = {2018},
  pages    = {e10},
  file     = {Snapshot:/home/paulbricman/Zotero/storage/QMJSN9GY/building-blocks.html:text/html}
}

@article{wang_language_2020,
  title    = {Language {Models} are {Open} {Knowledge} {Graphs}},
  url      = {http://arxiv.org/abs/2010.11967},
  abstract = {This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pretrained language models (without ﬁne-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.},
  language = {en},
  urldate  = {2022-02-15},
  journal  = {arXiv:2010.11967 [cs]},
  author   = {Wang, Chenguang and Liu, Xiao and Song, Dawn},
  month    = oct,
  year     = {2020},
  note     = {arXiv: 2010.11967},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  annote   = {Comment: 30 pages, 32 figures, 3 tables},
  file     = {Wang et al. - 2020 - Language Models are Open Knowledge Graphs.pdf:/home/paulbricman/Zotero/storage/ZNE2THYB/Wang et al. - 2020 - Language Models are Open Knowledge Graphs.pdf:application/pdf}
}

@article{storks_recent_2020,
  title      = {Recent {Advances} in {Natural} {Language} {Inference}: {A} {Survey} of {Benchmarks}, {Resources}, and {Approaches}},
  shorttitle = {Recent {Advances} in {Natural} {Language} {Inference}},
  url        = {http://arxiv.org/abs/1904.01172},
  abstract   = {In the NLP community, recent years have seen a surge of research activities that address machines’ ability to perform deep language understanding which goes beyond what is explicitly stated in text, rather relying on reasoning and knowledge of the world. Many benchmark tasks and datasets have been created to support the development and evaluation of such natural language inference ability. As these benchmarks become instrumental and a driving force for the NLP research community, this paper aims to provide an overview of recent benchmarks, relevant knowledge resources, and state-of-the-art learning and inference approaches in order to support a better understanding of this growing ﬁeld.},
  language   = {en},
  urldate    = {2022-02-15},
  journal    = {arXiv:1904.01172 [cs]},
  author     = {Storks, Shane and Gao, Qiaozi and Chai, Joyce Y.},
  month      = feb,
  year       = {2020},
  note       = {arXiv: 1904.01172},
  keywords   = {Computer Science - Computation and Language},
  file       = {Storks et al. - 2020 - Recent Advances in Natural Language Inference A S.pdf:/home/paulbricman/Zotero/storage/IE8AY8NN/Storks et al. - 2020 - Recent Advances in Natural Language Inference A S.pdf:application/pdf}
}

@article{swamy_interpreting_2021,
  title    = {Interpreting {Language} {Models} {Through} {Knowledge} {Graph} {Extraction}},
  url      = {http://arxiv.org/abs/2111.08546},
  abstract = {Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze "ﬁll-in-the-blank" statements at various stages of RoBERTa’s early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models’ behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance.},
  language = {en},
  urldate  = {2022-02-23},
  journal  = {arXiv:2111.08546 [cs]},
  author   = {Swamy, Vinitra and Romanou, Angelika and Jaggi, Martin},
  month    = nov,
  year     = {2021},
  note     = {arXiv: 2111.08546},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote   = {Comment: Published at NeurIPS 2021: eXplainable AI for Debugging and Diagnosis Workshop},
  file     = {Swamy et al. - 2021 - Interpreting Language Models Through Knowledge Gra.pdf:/home/paulbricman/Zotero/storage/WKXXNRFT/Swamy et al. - 2021 - Interpreting Language Models Through Knowledge Gra.pdf:application/pdf}
}

@inproceedings{toral_attaining_2018,
  address    = {Belgium, Brussels},
  title      = {Attaining the {Unattainable}? {Reassessing} {Claims} of {Human} {Parity} in {Neural} {Machine} {Translation}},
  shorttitle = {Attaining the {Unattainable}?},
  url        = {http://aclweb.org/anthology/W18-6312},
  doi        = {10.18653/v1/W18-6312},
  abstract   = {We reassess a recent study (Hassan et al., 2018) that claimed that machine translation (MT) has reached human parity for the translation of news from Chinese into English, using pairwise ranking and considering three variables that were not taken into account in that previous study: the language in which the source side of the test set was originally written, the translation proﬁciency of the evaluators, and the provision of inter-sentential context. If we consider only original source text (i.e. not translated from another language, or translationese), then we ﬁnd evidence showing that human parity has not been achieved. We compare the judgments of professional translators against those of non-experts and discover that those of the experts result in higher inter-annotator agreement and better discrimination between human and machine translations. In addition, we analyse the human translations of the test set and identify important translation issues. Finally, based on these ﬁndings, we provide a set of recommendations for future human evaluations of MT.},
  language   = {en},
  urldate    = {2022-05-18},
  booktitle  = {Proceedings of the {Third} {Conference} on {Machine} {Translation}: {Research} {Papers}},
  publisher  = {Association for Computational Linguistics},
  author     = {Toral, Antonio and Castilho, Sheila and Hu, Ke and Way, Andy},
  year       = {2018},
  pages      = {113--123},
  file       = {Toral et al. - 2018 - Attaining the Unattainable Reassessing Claims of .pdf:/home/paulbricman/Zotero/storage/XPTKSVJ7/Toral et al. - 2018 - Attaining the Unattainable Reassessing Claims of .pdf:application/pdf}
}

@article{ravuri_skilful_2021,
  title     = {Skilful precipitation nowcasting using deep generative models of radar},
  volume    = {597},
  copyright = {2021 The Author(s)},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/s41586-021-03854-z},
  doi       = {10.1038/s41586-021-03854-z},
  abstract  = {Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making1,2. State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations3,4. Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints5,6. While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536 km × 1,280 km and with lead times from 5–90 min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89\% of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
  language  = {en},
  number    = {7878},
  urldate   = {2022-05-18},
  journal   = {Nature},
  author    = {Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and Prudden, Rachel and Mandhane, Amol and Clark, Aidan and Brock, Andrew and Simonyan, Karen and Hadsell, Raia and Robinson, Niall and Clancy, Ellen and Arribas, Alberto and Mohamed, Shakir},
  month     = sep,
  year      = {2021},
  note      = {Number: 7878
               Publisher: Nature Publishing Group},
  keywords  = {Computer science, Environmental sciences},
  pages     = {672--677},
  file      = {Ravuri et al_2021_Skilful precipitation nowcasting using deep generative models of radar.pdf:/home/paulbricman/Zotero/storage/6CV2UVWP/Ravuri et al_2021_Skilful precipitation nowcasting using deep generative models of radar.pdf:application/pdf;Snapshot:/home/paulbricman/Zotero/storage/BNVMI5T8/s41586-021-03854-z.html:text/html}
}

@article{schrittwieser_mastering_2020,
  title     = {Mastering {Atari}, {Go}, chess and shogi by planning with a learned model},
  volume    = {588},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/s41586-020-03051-4},
  doi       = {10.1038/s41586-020-03051-4},
  abstract  = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game. A reinforcement-learning algorithm that combines a tree-based search with a learned model achieves superhuman performance in high-performance planning and visually complex domains, without any knowledge of their underlying dynamics.},
  language  = {en},
  number    = {7839},
  urldate   = {2022-05-18},
  journal   = {Nature},
  author    = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  month     = dec,
  year      = {2020},
  note      = {Number: 7839
               Publisher: Nature Publishing Group},
  keywords  = {Computer science, Computational science},
  pages     = {604--609},
  file      = {Schrittwieser et al_2020_Mastering Atari, Go, chess and shogi by planning with a learned model.pdf:/home/paulbricman/Zotero/storage/V24QDUMQ/Schrittwieser et al_2020_Mastering Atari, Go, chess and shogi by planning with a learned model.pdf:application/pdf;Snapshot:/home/paulbricman/Zotero/storage/9ZB3BIST/s41586-020-03051-4.html:text/html}
}

@article{degrave_magnetic_2022,
  title     = {Magnetic control of tokamak plasmas through deep reinforcement learning},
  volume    = {602},
  copyright = {2022 The Author(s)},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/s41586-021-04301-9},
  doi       = {10.1038/s41586-021-04301-9},
  abstract  = {Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak à Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and ‘snowflake’ configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained ‘droplets’ on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  language  = {en},
  number    = {7897},
  urldate   = {2022-05-18},
  journal   = {Nature},
  author    = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and de las Casas, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  month     = feb,
  year      = {2022},
  note      = {Number: 7897
               Publisher: Nature Publishing Group},
  keywords  = {Computer science, Magnetically confined plasmas, Nuclear fusion and fission},
  pages     = {414--419},
  file      = {Degrave et al_2022_Magnetic control of tokamak plasmas through deep reinforcement learning.pdf:/home/paulbricman/Zotero/storage/M6SRBCJA/Degrave et al_2022_Magnetic control of tokamak plasmas through deep reinforcement learning.pdf:application/pdf;Snapshot:/home/paulbricman/Zotero/storage/A832RWA7/s41586-021-04301-9.html:text/html}
}

@article{lecun_deep_2015,
  title     = {Deep learning},
  volume    = {521},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/nature14539},
  doi       = {10.1038/nature14539},
  abstract  = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  language  = {en},
  number    = {7553},
  urldate   = {2022-05-18},
  journal   = {Nature},
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  month     = may,
  year      = {2015},
  note      = {Number: 7553
               Publisher: Nature Publishing Group},
  keywords  = {Computer science, Mathematics and computing},
  pages     = {436--444}
}

@misc{noauthor_state---art_nodate,
  title    = {State-of-the-{Art} {Speech} {Recognition} with {Sequence}-to-{Sequence} {Models}},
  url      = {https://ieeexplore.ieee.org/document/8462105/},
  abstract = {Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-the-art ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2\% to 5.6\%, while the best conventional system achieves 6.7\%; on a dictation task our model achieves a WER of 4.1\% compared to 5\% for the conventional system.},
  language = {en-US},
  urldate  = {2022-05-18}
}

@techreport{radford_learning_2021,
  title       = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
  url         = {http://arxiv.org/abs/2103.00020},
  abstract    = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  number      = {arXiv:2103.00020},
  urldate     = {2022-05-18},
  institution = {arXiv},
  author      = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  month       = feb,
  year        = {2021},
  doi         = {10.48550/arXiv.2103.00020},
  note        = {arXiv:2103.00020 [cs]
                 type: article},
  keywords    = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file        = {arXiv.org Snapshot:/home/paulbricman/Zotero/storage/DRW8UQNV/2103.html:text/html;Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf:/home/paulbricman/Zotero/storage/DC235FL9/Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf:application/pdf}
}

@article{radford_improving_nodate,
  title    = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  language = {en},
  author   = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  pages    = {12},
  file     = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/home/paulbricman/Zotero/storage/WK3GIUGF/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf}
}

@inproceedings{tenney_bert_2019,
  address   = {Florence, Italy},
  title     = {{BERT} {Rediscovers} the {Classical} {NLP} {Pipeline}},
  url       = {https://www.aclweb.org/anthology/P19-1452},
  doi       = {10.18653/v1/P19-1452},
  abstract  = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We ﬁnd that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations.},
  language  = {en},
  urldate   = {2022-05-18},
  booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
  publisher = {Association for Computational Linguistics},
  author    = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  year      = {2019},
  pages     = {4593--4601},
  file      = {Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:/home/paulbricman/Zotero/storage/VGK6GXW9/Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:application/pdf}
}

@misc{arrieta_explainable_2019,
  title      = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, {Taxonomies}, {Opportunities} and {Challenges} toward {Responsible} {AI}},
  shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
  url        = {http://arxiv.org/abs/1910.10045},
  abstract   = {In the last few years, Artiﬁcial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the ﬁeld. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) ﬁeld, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the ﬁeld of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to deﬁne explainability in Machine Learning, establishing a novel deﬁnition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this deﬁnition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artiﬁcial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the ﬁeld of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the beneﬁts of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  language   = {en},
  urldate    = {2022-05-18},
  publisher  = {arXiv},
  author     = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  month      = dec,
  year       = {2019},
  note       = {Number: arXiv:1910.10045
                arXiv:1910.10045 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
  annote     = {Comment: 67 pages, 13 figures, accepted for its publication in Information Fusion},
  file       = {Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:/home/paulbricman/Zotero/storage/3986Y46S/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf}
}

@misc{koehn_neural_2017,
  title     = {Neural {Machine} {Translation}},
  url       = {http://arxiv.org/abs/1709.07809},
  abstract  = {Draft of textbook chapter on neural machine translation. a comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges. Written as chapter for the textbook Statistical Machine Translation. Used in the JHU Fall 2017 class on machine translation.},
  language  = {en},
  urldate   = {2022-05-18},
  publisher = {arXiv},
  author    = {Koehn, Philipp},
  month     = sep,
  year      = {2017},
  note      = {Number: arXiv:1709.07809
               arXiv:1709.07809 [cs]},
  keywords  = {Computer Science - Computation and Language},
  annote    = {Comment: 100+ pages},
  file      = {Koehn - 2017 - Neural Machine Translation.pdf:/home/paulbricman/Zotero/storage/I9YKFNLI/Koehn - 2017 - Neural Machine Translation.pdf:application/pdf}
}

@misc{dosovitskiy_image_2021,
  title      = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
  shorttitle = {An {Image} is {Worth} 16x16 {Words}},
  url        = {http://arxiv.org/abs/2010.11929},
  abstract   = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  language   = {en},
  urldate    = {2022-05-18},
  publisher  = {arXiv},
  author     = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  month      = jun,
  year       = {2021},
  note       = {Number: arXiv:2010.11929
                arXiv:2010.11929 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  file       = {Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:/home/paulbricman/Zotero/storage/T8DG8TNM/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf}
}

@article{devlin_bert_nodate,
  title    = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
  language = {en},
  author   = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  pages    = {16},
  file     = {Devlin et al. - BERT Pre-training of Deep Bidirectional Transform.pdf:/home/paulbricman/Zotero/storage/XL5L2NFF/Devlin et al. - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@article{jaeger_using_nodate,
  title    = {Using {Conceptors} to {Manage} {Neural} {Long}-{Term} {Memories} for {Temporal} {Patterns}},
  abstract = {Biological brains can learn, recognize, organize, and re-generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called conceptors, which oﬀers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and ”focussed”. Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content-addressed in ways that are analog to recalling static patterns in Hopﬁeld networks.},
  language = {en},
  author   = {Jaeger, Herbert},
  pages    = {43},
  file     = {Jaeger - Using Conceptors to Manage Neural Long-Term Memori.pdf:/home/paulbricman/Zotero/storage/QCV3TG2L/Jaeger - Using Conceptors to Manage Neural Long-Term Memori.pdf:application/pdf}
}

@misc{jaeger_controlling_2017,
  title     = {Controlling {Recurrent} {Neural} {Networks} by {Conceptors}},
  url       = {http://arxiv.org/abs/1403.3369},
  abstract  = {The human brain is a dynamical system whose extremely complex sensordriven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientiﬁc challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically ﬁltered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks.},
  language  = {en},
  urldate   = {2022-05-18},
  publisher = {arXiv},
  author    = {Jaeger, Herbert},
  month     = apr,
  year      = {2017},
  note      = {Number: arXiv:1403.3369
               arXiv:1403.3369 [cs]},
  keywords  = {Computer Science - Neural and Evolutionary Computing, I.2.6},
  annote    = {Comment: 200 pages, 50 figures},
  file      = {Jaeger - 2017 - Controlling Recurrent Neural Networks by Conceptor.pdf:/home/paulbricman/Zotero/storage/GWN6HXQW/Jaeger - 2017 - Controlling Recurrent Neural Networks by Conceptor.pdf:application/pdf}
}

@misc{liu_roberta_2019,
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  language   = {en},
  urldate    = {2022-05-18},
  publisher  = {arXiv},
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  month      = jul,
  year       = {2019},
  note       = {Number: arXiv:1907.11692
                arXiv:1907.11692 [cs]},
  keywords   = {Computer Science - Computation and Language},
  file       = {Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:/home/paulbricman/Zotero/storage/IHMKUSGN/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf}
}

@misc{lan_albert_2020,
  title      = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
  shorttitle = {{ALBERT}},
  url        = {http://arxiv.org/abs/1909.11942},
  abstract   = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  language   = {en},
  urldate    = {2022-05-18},
  publisher  = {arXiv},
  author     = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  month      = feb,
  year       = {2020},
  note       = {Number: arXiv:1909.11942
                arXiv:1909.11942 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  file       = {Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:/home/paulbricman/Zotero/storage/LJRUNL2X/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:application/pdf}
}

@misc{sanh_distilbert_2020,
  title      = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  shorttitle = {{DistilBERT}, a distilled version of {BERT}},
  url        = {http://arxiv.org/abs/1910.01108},
  abstract   = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-speciﬁc models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  language   = {en},
  urldate    = {2022-05-18},
  publisher  = {arXiv},
  author     = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  month      = feb,
  year       = {2020},
  note       = {Number: arXiv:1910.01108
                arXiv:1910.01108 [cs]},
  keywords   = {Computer Science - Computation and Language},
  annote     = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
  file       = {Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:/home/paulbricman/Zotero/storage/EQUH6ZW6/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf}
}

@misc{he_deberta_2021,
  title      = {{DeBERTa}: {Decoding}-enhanced {BERT} with {Disentangled} {Attention}},
  shorttitle = {{DeBERTa}},
  url        = {http://arxiv.org/abs/2006.03654},
  abstract   = {Recent progress in pre-trained neural language models has signiﬁcantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The ﬁrst is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for ﬁne-tuning to improve models’ generalization. We show that these techniques signiﬁcantly improve the efﬁciency of model pre-training and the performance of both natural language understand (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The signiﬁcant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the ﬁrst time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). The pre-trained DeBERTa models and the source code were released at: https://github.com/microsoft/DeBERTa1.},
  language   = {en},
  urldate    = {2022-05-18},
  publisher  = {arXiv},
  author     = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  month      = oct,
  year       = {2021},
  note       = {Number: arXiv:2006.03654
                arXiv:2006.03654 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, cs.CL, cs.GL, I.2, I.7},
  annote     = {Comment: 20 pages,5 figures, 13 tables. In v2, we scale up DeBERTa to 1.5B parameters and it surpasses the human performance on SuperGLUE leaderboard for the first time as of December 29, 2020. In v3, we replace MLM with RTD objective which significantly improves the model performance},
  file       = {He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf:/home/paulbricman/Zotero/storage/ZEG2WQ9Q/He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf:application/pdf}
}

@misc{bolukbasi_man_2016,
  title      = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
  shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
  url        = {http://arxiv.org/abs/1607.06520},
  abstract   = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is ﬁrst shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender deﬁnition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We deﬁne metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to “debias” the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms signiﬁcantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
  language   = {en},
  urldate    = {2022-05-18},
  publisher  = {arXiv},
  author     = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  month      = jul,
  year       = {2016},
  note       = {Number: arXiv:1607.06520
                arXiv:1607.06520 [cs, stat]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
  file       = {Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:/home/paulbricman/Zotero/storage/4RVIXKIB/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf}
}

@article{danilevsky_survey_nodate,
  title    = {A {Survey} of the {State} of {Explainable} {AI} for {Natural} {Language} {Processing}},
  abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
  language = {en},
  author   = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  pages    = {13},
  file     = {Danilevsky et al. - A Survey of the State of Explainable AI for Natura.pdf:/home/paulbricman/Zotero/storage/R94E7Z8H/Danilevsky et al. - A Survey of the State of Explainable AI for Natura.pdf:application/pdf}
}

@inproceedings{liu_interpretation_2018,
  address   = {New York, NY, USA},
  series    = {{KDD} '18},
  title     = {On {Interpretation} of {Network} {Embedding} via {Taxonomy} {Induction}},
  isbn      = {978-1-4503-5552-0},
  url       = {https://doi.org/10.1145/3219819.3220001},
  doi       = {10.1145/3219819.3220001},
  abstract  = {Network embedding has been increasingly used in many network analytics applications to generate low-dimensional vector representations, so that many off-the-shelf models can be applied to solve a wide variety of data mining tasks. However, similar to many other machine learning methods, network embedding results remain hard to be understood by users. Each dimension in the embedding space usually does not have any specific meaning, thus it is difficult to comprehend how the embedding instances are distributed in the reconstructed space. In addition, heterogeneous content information may be incorporated into network embedding, so it is challenging to specify which source of information is effective in generating the embedding results. In this paper, we investigate the interpretation of network embedding, aiming to understand how instances are distributed in embedding space, as well as explore the factors that lead to the embedding results. We resort to the post-hoc interpretation scheme, so that our approach can be applied to different types of embedding methods. Specifically, the interpretation of network embedding is presented in the form of a taxonomy. Effective objectives and corresponding algorithms are developed towards building the taxonomy. We also design several metrics to evaluate interpretation results. Experiments on real-world datasets from different domains demonstrate that, by comparing with the state-of-the-art alternatives, our approach produces effective and meaningful interpretation to embedding results.},
  urldate   = {2022-05-18},
  booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
  publisher = {Association for Computing Machinery},
  author    = {Liu, Ninghao and Huang, Xiao and Li, Jundong and Hu, Xia},
  month     = jul,
  year      = {2018},
  keywords  = {machine learning interpretation, network embedding, taxonomy},
  pages     = {1812--1820}
}

@misc{hinton_how_2021,
  title     = {How to represent part-whole hierarchies in a neural network},
  url       = {http://arxiv.org/abs/2102.12627},
  abstract  = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several diﬀerent groups to be combined into an imaginary system called GLOM1. The advances include transformers, neural ﬁelds, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a ﬁxed architecture parse an image into a partwhole hierarchy which has a diﬀerent structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should signiﬁcantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.},
  language  = {en},
  urldate   = {2022-05-18},
  publisher = {arXiv},
  author    = {Hinton, Geoffrey},
  month     = feb,
  year      = {2021},
  note      = {Number: arXiv:2102.12627
               arXiv:2102.12627 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, I.2.6, I.4.8},
  annote    = {Comment: 43 pages, 5 figures},
  file      = {Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf:/home/paulbricman/Zotero/storage/LWRNMG8K/Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf:application/pdf}
}

@article{jain_attention_nodate,
  title    = {Attention is not {Explanation}},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations" for predictions. We ﬁnd that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our ﬁndings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.},
  language = {en},
  author   = {Jain, Sarthak and Wallace, Byron C},
  pages    = {14},
  file     = {Jain and Wallace - Attention is not Explanation.pdf:/home/paulbricman/Zotero/storage/LSRDLPP7/Jain and Wallace - Attention is not Explanation.pdf:application/pdf}
}

@misc{noauthor_multimodal_nodate,
  title   = {Multimodal {Neurons} in {Artificial} {Neural} {Networks}},
  url     = {https://distill.pub/2021/multimodal-neurons/},
  urldate = {2022-05-18},
  file    = {Multimodal Neurons in Artificial Neural Networks:/home/paulbricman/Zotero/storage/P8T3TSY7/multimodal-neurons.html:text/html}
}

@techreport{reimers_sentence-bert_2019,
  title       = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
  shorttitle  = {Sentence-{BERT}},
  url         = {http://arxiv.org/abs/1908.10084},
  abstract    = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  number      = {arXiv:1908.10084},
  urldate     = {2022-05-19},
  institution = {arXiv},
  author      = {Reimers, Nils and Gurevych, Iryna},
  month       = aug,
  year        = {2019},
  doi         = {10.48550/arXiv.1908.10084},
  note        = {arXiv:1908.10084 [cs]
                 type: article},
  keywords    = {Computer Science - Computation and Language},
  annote      = {Comment: Published at EMNLP 2019},
  file        = {arXiv.org Snapshot:/home/paulbricman/Zotero/storage/VN4HH2M5/1908.html:text/html;Reimers_Gurevych_2019_Sentence-BERT.pdf:/home/paulbricman/Zotero/storage/TPKHE7DJ/Reimers_Gurevych_2019_Sentence-BERT.pdf:application/pdf}
}

@techreport{yin_benchmarking_2019,
  title       = {Benchmarking {Zero}-shot {Text} {Classification}: {Datasets}, {Evaluation} and {Entailment} {Approach}},
  shorttitle  = {Benchmarking {Zero}-shot {Text} {Classification}},
  url         = {http://arxiv.org/abs/1909.00161},
  abstract    = {Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects: the ``topic'' aspect includes ``sports'' and ``politics'' as labels; the ``emotion'' aspect includes ``joy'' and ``anger''; the ``situation'' aspect includes ``medical assistance'' and ``water shortage''. ii) We extend the existing evaluation setup (label-partially-unseen) -- given a dataset, train on some labels, test on all labels -- to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way. Code \& Data: https://github.com/yinwenpeng/BenchmarkingZeroShot},
  number      = {arXiv:1909.00161},
  urldate     = {2022-05-19},
  institution = {arXiv},
  author      = {Yin, Wenpeng and Hay, Jamaal and Roth, Dan},
  month       = aug,
  year        = {2019},
  doi         = {10.48550/arXiv.1909.00161},
  note        = {arXiv:1909.00161 [cs]
                 type: article},
  keywords    = {Computer Science - Computation and Language},
  annote      = {Comment: EMNLP2019 camera-ready, 10 pages},
  file        = {arXiv.org Snapshot:/home/paulbricman/Zotero/storage/YH7S5H7P/1909.html:text/html;Yin et al_2019_Benchmarking Zero-shot Text Classification.pdf:/home/paulbricman/Zotero/storage/KSUGWRZS/Yin et al_2019_Benchmarking Zero-shot Text Classification.pdf:application/pdf}
}

@article{jiang_evaluating_2019,
  title      = {Evaluating {BERT} for natural language inference: {A} case study on the {CommitmentBank}},
  shorttitle = {Evaluating {BERT} for natural language inference},
  url        = {https://par.nsf.gov/biblio/10158557-evaluating-bert-natural-language-inference-case-study-commitmentbank},
  doi        = {10.18653/v1/D19-1630},
  abstract   = {Natural language inference (NLI) datasets (e.g., MultiNLI) were collected by soliciting hypotheses for a given premise from annotators. Such data collection led to annotation artifacts: systems can identify the premise-hypothesis relationship without observing the premise (e.g., negation in hypothesis being indicative of contradiction). We address this problem by recasting the CommitmentBank for NLI, which contains items involving reasoning over the extent to which a speaker is committed to complements of clause-embedding verbs under entailment-canceling environments (conditional, negation, modal and question). Instead of being constructed to stand in certain relationships with the premise, hypotheses in the recast CommitmentBank are the complements of the clause-embedding verb in each premise, leading to no annotation artifacts in the hypothesis. A state-of-the-art BERT-based model performs well on the CommitmentBank with 85\% F1. However analysis of model behavior shows that the BERT models still do not capture the full complexity of pragmatic reasoning, nor encode some of the linguistic generalizations, highlighting room for improvement.},
  language   = {en},
  urldate    = {2022-05-19},
  journal    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  author     = {Jiang, Nanjiang and de Marneffe, Marie-Catherine},
  month      = jan,
  year       = {2019},
  file       = {Jiang_de Marneffe_2019_Evaluating BERT for natural language inference.pdf:/home/paulbricman/Zotero/storage/BJPIREN2/Jiang_de Marneffe_2019_Evaluating BERT for natural language inference.pdf:application/pdf;Snapshot:/home/paulbricman/Zotero/storage/EVYPSBRF/10158557.html:text/html}
}

@misc{west_symbolic_2021,
  title      = {Symbolic {Knowledge} {Distillation}: from {General} {Language} {Models} to {Commonsense} {Models}},
  shorttitle = {Symbolic {Knowledge} {Distillation}},
  url        = {http://arxiv.org/abs/2110.07178},
  abstract   = {The common practice for training commonsense models has gone from–human–to–corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus–to–machine: general language models author these commonsense knowledge graphs to train commonsense models.},
  language   = {en},
  urldate    = {2022-05-19},
  publisher  = {arXiv},
  author     = {West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena D. and Jiang, Liwei and Bras, Ronan Le and Lu, Ximing and Welleck, Sean and Choi, Yejin},
  month      = oct,
  year       = {2021},
  note       = {Number: arXiv:2110.07178
                arXiv:2110.07178 [cs]},
  keywords   = {Computer Science - Computation and Language},
  file       = {West et al. - 2021 - Symbolic Knowledge Distillation from General Lang.pdf:/home/paulbricman/Zotero/storage/3JEAQVZ3/West et al. - 2021 - Symbolic Knowledge Distillation from General Lang.pdf:application/pdf}
}
