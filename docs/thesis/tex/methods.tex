\section{Methods}\label{sec:methods}

\subsection{Model}

\textbf{As the object of our interpretation technique, we chose a pretrained BERT model, short for Bidirectional Encoder Representations from Transformers. For completeness, BERT is a transformer model which maps a set of subword tokens to another set of such tokens. BERT has been originally trained on two different natural language processing objectives. First, it has been tasked with a masked language modeling (MLM) objective. This refers to the task of reconstructing a short input text which has been intentionally corrupted. The corruption typically consists in eliminating (i.e. masking) a random proportion of the tokens contained in the input text (e.g. "BERT is a transformer model." might be corrupted as "BERT is a [MASK] model."). Given this, the MLM task consists in reconstructing the pre-corruption text from the corrupted version.}

\textbf{The second objective employed in training BERT is a next sentence prediction (NSP) task. Given a pair of two sentences, BERT is tasked with predicting whether they are consecutive in the original text. The combination of those two conceptually simple objectives has been shown to help BERT learn rich semantic representations of the text being processed, as an instrumental goal in solving the two tasks. For instance, mean-pooling token embeddings across texts has been shown to be highly effective in downstream information retrieval tasks based on vector similarity. Moreover, mean-pooling token embeddings of a text and comparing the result with the mean-pooled embeddings of a set of labels (e.g. science, politics, economics), has been shown to be a competitive baseline in text classification. Alternatively, BERT models fine-tuned on limited data from other tasks (e.g. natural language inference) had yielded state-of-the-art performance in multiple tasks.}

\textbf{Internally, BERT represents each token as an embedding of dimensionality 768. As the model consists of a repetitive sequence of layers, the set of embeddings which represents tokens is adjusted from one layer to the next using multi-head attention mechanisms. It is precisely those token embeddings which we are trying to distill knowledge graphs from using the NSC approach. Specifically, as often done in prior art, we are attempting to interpret the set of embeddings which are generated in the \textit{last} BERT layer. This has been hypothesized to contain high-level features extracted the input tokens which are based in large part on the tokens' contexts, after extensive processing in the earlier layers of the model.}

We opted for BERT due to its widespread use in industry applications and its large number of derivative models (e.g. RoBERTa, ALBERT, distilBERT, etc.). BERT takes in a sequence of subword tokens as input and reconstructs it as output, while generating a unique contextual embedding for each token. Crucially, the same token can be attributed different embeddings in different contexts (e.g. "she" referring to different people). In practice, the contextual embeddings of individual tokens are mean-pooled together to yield an overarching document embedding. However, here we focus only on the contextual embeddings of individual tokens or at most short sequences of them which form a noun phrase (e.g. "orange juice").

\subsection{Data}

As NSC requires an auxiliary dataset for generating state clouds of contextual embeddings, we employ one of the datasets which have been used for training the BERT model, namely BookCorpus. This dataset consists of a variety of public domain books across different genres, and provides many different contexts for tokens to appear in.

We focus our investigation on relating a set of 100 hand-picked concepts to each other. For each concept, we extract all contexts in which they appear verbatim in the dataset. A context is defined as the span of text starting 300 characters before and ending 300 characters after the concept occurence. Additionally, we trim the incomplete beginning and ending sentences (i.e. trailing) from each context, leaving in only complete sentences surrounding the concept occurence.

For each context, we extract the contextual embedding of the concept occurence, obtaining a set of such embeddings for each concept. The cardinality of each set depends on the frequency of occurence of the concept in the dataset. We further filter our set of concepts based on the size of the set of contextual embeddings, eliminating concepts which had fewer occurences than the number of BERT embedding dimensions (i.e. 768). We cover difficulties in handling sparser state clouds in the discussion.

\subsection{Conceptors}

\textbf{From each remaining state cloud representing the set of concept nuances used in the dataset, we learn a conceptor. For completeness, a conceptor is a mathematical object which models the distribution of state cloud in its space. However, conceptors do \textit{not} represent the \textit{density} of embeddings in space using a probability density function. Rather, a conceptor represents the \textit{dimensions} across which the state cloud spreads most across space, together with the spread associated with each dimension. This information can be compactly represented in a square matrix whose dimensionality matches the one of the space populated by the state cloud.}

\textbf{Both conceptors and Principal Component Analysis (PCA) make use of the correlation matrix of the state cloud. However, an additional parameter appears in the case of conceptors. Specifically, a conceptor also requires an aperture to be defined. The aperture is a parameter which dictates the extent to which the eccentricity of the state cloud should be reflected in the associated conceptor object.}

based on the closed-form equation introduced in previous work. Each conceptor is represented through a square matrix whose dimensions match the dimensionality of BERT embeddings (i.e. 768), representing the PCA-like directions spanned by the state cloud, yet mediated by an additional aperture parameter. Additionally, we note that for rich state clouds based on relatively frequent concepts (e.g. "water"), the conceptor representation resulted in three orders-of-magnitude smaller memory footprint compared to storing the original set of contextual embeddings.

\begin{align*}
    C(R, \alpha) = R (R + \alpha^{-2} I)^{-1}
\end{align*}

\subsection{Abstraction Ordering}

For each pair of conceptors learned from state clouds, we estimate their relation of abstraction based on a heuristic. First, we substract one conceptor matrix from the other. Second, we compute the mean of the eigenvalues the difference matrix. Intuitively, all eigenvalues of the difference matrix are positive if the first conceptor spatially engulfs the other, having higher spread than the second across all dimensions. Conversely, all such eigenvalues are negative if the first conceptor is completely contained by the second across all dimensions. Inevitably, however, the two conceptors will exhibit one such relation across \textit{some} of the 768 dimensions, while simultaneously exhibiting the opposite in other dimensions. Hence, we average the eigenvalues in an attempt to reach a "consensus" opinion on how the two conceptors are related to each other:

\begin{align*}
    f(C_1, C_2) = \frac{1}{n} \sum\limits_{i=1}^n \lambda_i(C_1 - C_2)
\end{align*}

Where $C_1$ and $C_2$ are conceptor matrices, and \textit{n} is the number of dimensions in the difference matrix.

\subsection{Graph Optimization}

Given the pairwise estimates of abstraction ordering computed before, we conduct a graph optimization process. All candidate graphs considered are directed ones, while nodes are identified with concepts, and arcs indicate meronymous relations of abstraction (i.e. IS\_A).

We attempt to solve the graph optimization task through the local search algorithm of simulated annealing (see \ref{alg:gs}). \textbf{Each candidate graph is represented through a Boolean adjacency matrix $A$. Specifically, $A^k$ denotes the adjacency matrix of the candidate graph considered at step $k$ of the graph optimization process. $A^k_{ij}$ is a Boolean value indicating whether concept $i$ links to concept $j$ in the associated candidate graph of step $k$.} As an initial candidate graph, the optimization process starts with a fully-disconnected graph, where no concepts are related to each other. This is represented through an adjacency matrix full of null values, $A^0 = 0$. Then, we randomly sample a new graph proposal by randomly mutating the current graph -- removing a previous arc or adding a new one. The acceptance probability is informed by a temperature schedule which linearly decreases from one to zero over the course of the search process, encouraging heavy exploration in the first epochs while using an increasingly conservative strategy towards the end.

\textbf{The objective function which the search algorithm attempts to maximize is a linear combination of four different terms. Each term is a function of either or both (1) the adjacency matrix $A^i$ which is identified with the state of the graph optimization process in step $i$, and (2) the matrix $D$ containing pairwise estimates of abstract ordering. $D_{ij}$ denotes the numerical estimate of abstraction between conceptor $i$ and $j$. In other words, $$D_{ij} = f(C_i, C_j)$$. We note that the particular way $D_{ij}$ is related to $D_{ji}$ is determined by the choice numerical heuristic employed for abstraction ordering. In our case (i.e. mean of eigenvalues of difference matrix), $D_{ij} = -D_{ji}$, yet this is not necessarily the case when opting for other heuristics, as explored in the discussion (e.g. positive-negative eigenvalues ratio). Besides the two matrices just described which influence the objective function through the four terms whose description follows, the objective function is also influenced by the four coefficients which are used to weigh the four terms.}

\textbf{The first term is a function of both $A^i$ and $D$. It is equal to the mean of the abstraction ordering estimates represented in the candidate graph by means of arcs. From now on, we refer to this term as \textit{expressed abstraction} (EA). In case of a fully-disconnected graph (i.e. one in which no arc exists in the graph at all) represented by $A^i = 0$, $EA(A^i, D) = 0$. In contrast, in case of a fully-connected graph (i.e. one in which there exists an arc between any two nodes) represented by $A^i = 1$, $$EA(A^i, D) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij} D_{ij}.$$ In general, $min(D) \leq EA(A^i, D) \leq max(D)$. This is the only among the four terms of the linear combination which indicates functional-groundedness, as it reflects the proportion of the abstraction identified in high-dimensional space which gets represented in the output knowledge graph.}

\textbf{The second term is only a function of $A^i$. It is equal to the proportion of arcs contained by the candidate graph represented by adjacency matrix $A^i$, relative to the maximum number of possible arcs $n^2$: $$AD(A^i) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij}.$$ We refer to this term as \textit{arc density} (AD). In case of a fully-disconnected graph with $A^i = 0$, $AD(A^i) = 0$. In case of a fully-connected graph with $A^i = 1$, $AD(A^i) = 1$. In all other cases, $0 < AD(A^i) < 1$.}

\textbf{The third term is also only a function of $A^i$. It is equal to the mean difference between a node's children count (i.e. number of nodes connected via outbound arcs) and a target children count set in advance: $$CE(A^i) = \frac{1}{n} \sum_{i=1}^{n} \left|(\sum_{j=1}^{n}A_{ij}) - target\_children\right|.$$ We refer to this term as \textit{children error} (CE). In case of a graph with adjacency matrix $A^i$ in which every node only has children equal in count to $target\_children$, $CE(A^i)=0$. For all other graphs, $CE(A^i) > 0$.}

\textbf{The fourth and final term of the objective function is extremely similar to the previous one, yet it addresses the distribution of parent counts, rather than children counts. Concretely, it is equal to the mean difference between a node's parent count (i.e. number of nodes connected via inbound arcs) and a target parent count set in advance: $$PE(A^i) = \frac{1}{n} \sum_{j=1}^{n} \left|(\sum_{i=1}^{n}A_{ij}) - target\_parents\right|.$$ We refer to this term as \textit{parent error} (PE). In case of a graph with adjacency matrix $A^i$ in which every node only has parents equal in count to $target\_parents$, $PE(A^i)=0$. For all other graphs, $PE(A^i) > 0$.}

\textbf{Besides the four terms which are functions of either or both $A^i$ and $D$, the objective function also contain four coefficients meant to influence the relative weight of each term. We denote these as $\alpha, \beta, \gamma$, and $\delta$ in order for the four terms. Given the four terms, the two additional targets for child and parent count, and the four weighing coefficients included in the linear combination, the objective function can finally be defined as:}


\begin{align*}
    & score(A^i, D, target\_children, target\_parents) = \\
    & \alpha EA(A^i, D) \\
    & - \beta AD(A^i) \\
    & - \gamma CE(A^i, target\_children) \\
    & - \delta PE(A^i, target\_parents).
\end{align*}

\begin{algorithm}[!tbp] 
    \caption{Graph Search in NSC}
    \label{alg:gs}
    \begin{algorithmic}
        \STATE $s \Leftarrow 0 $ (fully-disconnected graph) \\
        \FOR{$k = 0$ to $epochs$}
        \STATE $T \Leftarrow 1 - \frac{k}{epochs}$ \\
        \STATE $s_{new} \Leftarrow neighbor(s)$ \\
        \IF{$P(score(D, s), score(D, s_{new}), T) \geq random(0, 1)$}
            \STATE $s \Leftarrow s_{new}$
        \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

The result of the graph search is the final output of NSC: a graph which indicates how the underlying ML model relates concepts by means of contextual embeddings.

\begin{algorithm}[!tbp] 
    \caption{Nested State Clouds}
    \label{alg:gs}
    \begin{algorithmic}
        \FOR{$s$ in $symbols$}
            \STATE $a \Leftarrow contexts(s)$ \\
            \STATE $b \Leftarrow cloud(a)$ \\
            \STATE $c \Leftarrow conceptor(b)$ \\
        \ENDFOR
        \FOR{$i, c_i$ in $conceptors$}
            \FOR{$j, c_j$ in $conceptors$}
                \STATE $D_{ij} \Leftarrow f(c_i, c_j)$
            \ENDFOR
        \ENDFOR
        \STATE $s_{output} \Leftarrow graph\_search(D)$
    \end{algorithmic}
\end{algorithm}