\section{Methods}\label{sec:methods}

\subsection{Model}

As the object of our interpretation technique, we chose a pretrained BERT model, due to its widespread use in industry applications and its large number of derivative models (e.g. RoBERTa, ALBERT, distilBERT, etc.). BERT takes in a sequence of subword tokens as input and reconstructs it as output, while generating a unique contextual embedding for each token. Crucially, the same token can be attributed different embeddings in different contexts (e.g. "she" referring to different people). In practice, the contextual embeddings of individual tokens are mean-pooled together to yield an overarching document embedding which enables information retrieval. However, here we focus only on the contextual embeddings of individual tokens or at most short sequences of them which form a noun phrase (e.g. "orange juice").

\subsection{Data}

As NSC requires an auxiliary dataset for generating state clouds of contextual embeddings, we employ one of the datasets which have been used for training the BERT model, namely BookCorpus. This dataset consists of a variety of public domain books across different genres, and provides many different contexts for tokens to appear in.

We focus our investigation on relating a set of 100 hand-picked concepts to each other. For each concept, we extract all contexts in which they appear verbatim in the dataset. A context is defined as the span of text starting 300 characters before and ending 300 characters after the concept occurence. Additionally, we trim the incomplete beginning and ending sentences (i.e. trailing) from each context, leaving in only complete sentences surrounding the concept occurence.

For each context, we extract the contextual embedding of the concept occurence, obtaining a set of such embeddings for each concept. The cardinality of each set depends on the frequency of occurence of the concept in the dataset. We further filter our set of concepts based on the size of the set of contextual embeddings, eliminating concepts which had fewer occurences than the number of BERT embedding dimensions (i.e. 768). We cover difficulties in handling sparser state clouds in the discussion.

\subsection{Conceptors}

From each remaining state cloud representing the set of concept nuances used in the dataset, we derived a conceptor based on the closed-form equation introduced in previous work. Each conceptor is represented through a square matrix whose dimensions match the dimensionality of BERT embeddings (i.e. 768), representing the PCA-like directions spanned by the state cloud, yet mediated by an additional aperture parameter. Additionally, we note that for rich state clouds based on relatively frequent concepts (e.g. "water"), the conceptor representation resulted in three orders-of-magnitude smaller memory footprint compared to storing the original set of contextual embeddings.

\begin{align*}
    C(R, \alpha) = R (R + \alpha^{-2} I)^{-1}
\end{align*}

\subsection{Abstraction Ordering}

For each pair of conceptors learned from state clouds, we estimate their relation of abstraction based on a heuristic. First, we substract one conceptor matrix from the other. Second, we compute the mean of the eigenvalues the difference matrix. Intuitively, all eigenvalues of the difference matrix are positive if the first conceptor spatially engulfs the other, having higher spread than the second across all dimensions. Conversely, all such eigenvalues are negative if the first conceptor is completely contained by the second across all dimensions. Inevitably, however, the two conceptors will exhibit one such relation across \textit{some} of the 768 dimensions, while simultaneously exhibiting the opposite in other dimensions. Hence, we average the eigenvalues in an attempt to reach a "consensus" opinion on how the two conceptors are related to each other:

\begin{align*}
    f(C_1, C_2) = \frac{1}{n} \sum\limits_{i=1}^n \lambda_i(C_1 - C_2)
\end{align*}

Where $C_1$ and $C_2$ are conceptor matrices, and \textit{n} is the number of dimensions in the difference matrix.

\subsection{Graph Search}

Given the pairwise estimates of abstraction ordering computed before, we conduct a graph search. We specifically note that we search \textit{for} a graph in the space of possible graphs, rather than searching for a path through a given graph. All candidate graphs considered are directed ones, while nodes are identified with concepts, and arcs indicate meronymous relations of abstraction (i.e. IS\_A).

We attempt to solve the graph search task through the local search algorithm of simulated annealing. As an initial candidate, we start with a completely disconnected graph, where no concepts are related to each other. This is represented through an adjacency matrix full of null values. Then, we randomly sample a new proposal by randomly mutating the current graph -- removing a previous arc or adding a new one. The acceptance probability is informed by a temperature schedule which linearly decreases from one to zero over the course of the search process, encouraging heavy exploration in the first epochs while using an increasingly conservative strategy towards the end.

\textbf{The objective function which the search algorithm attempts to maximize is a linear combination of four different terms. Each term is a function of either or both (1) the adjacency matrix $A^i$ which is identified with the state of the graph optimization process in step $i$, and (2) the matrix $D$ containing pairwise estimates of abstract ordering. $D_{ij}$ denotes the numerical estimate of abstraction between conceptor $i$ and $j$. We note that the particular way $D_{ij}$ is related to $D_{ji}$ is determined by the choice numerical heuristic employed for abstraction ordering. In our case (i.e. mean of eigenvalues of difference matrix), $D_{ij} = -D_{ji}$, yet this is not necessarily the case when opting for other heuristics, as explored in the discussion (e.g. positive-negative eigenvalues ratio). Besides the two matrices just described which influence the objective function through the four terms whose description follows, the objective function is also influenced by the four coefficients which are used to weigh the four terms.}

\textbf{The first term is a function of both $A^i$ and $D$. It is equal to the mean of the abstraction ordering estimates represented in the candidate graph by means of arcs. From now on, we refer to this term as \textit{expressed abstraction} (EA). In case of a fully-disconnected graph (i.e. one in which no arc exists in the graph at all) represented by $A^i = 0$, $EA(A^i, D) = 0$. In contrast, in case of a fully-connected graph (i.e. one in which there exists an arc between any two nodes) represented by $A^i = 1$, $$EA(A^i, D) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij} D_{ij}.$$ In general, $min(D) \leq EA(A^i, D) \leq max(D)$. This is the only among the four terms of the linear combination which indicates functional-groundedness, as it reflects the proportion of the abstraction identified in high-dimensional space which gets represented in the output knowledge graph.}

\textbf{The second term is only a function of $A^i$. It is equal to the proportion of arcs contained by the candidate graph represented by adjacency matrix $A^i$, relative to the maximum number of possible arcs $n^2$: $$AD(A^i) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij}.$$ We refer to this term as \textit{arc density} (AD). In case of a fully-disconnected graph with $A^i = 0$, $AD(A^i) = 0$. In case of a fully-connected graph with $A^i = 1$, $AD(A^i) = 1$. In all other cases, $0 < AD(A^i) < 1$.}

\textbf{The third term is also only a function of $A^i$. It is equal to the mean difference between a node's children count (i.e. number of nodes connected via outbound arcs) and a target children count set in advance: $$CE(A^i) = \frac{1}{n} \sum_{i=1}^{n} \left|(\sum_{j=1}^{n}A_{ij}) - target\_children\right|.$$ We refer to this term as \textit{children error} (CE). In case of a graph with adjacency matrix $A^i$ in which every node only has children equal in count to $target\_children$, $CE(A^i)=0$. For all other graphs, $CE(A^i) > 0$.}

\textbf{The fourth and final term of the objective function is extremely similar to the previous one, yet it addresses the distribution of parent counts, rather than children counts. Concretely, it is equal to the mean difference between a node's parent count (i.e. number of nodes connected via inbound arcs) and a target parent count set in advance: $$PE(A^i) = \frac{1}{n} \sum_{j=1}^{n} \left|(\sum_{i=1}^{n}A_{ij}) - target\_parents\right|.$$ We refer to this term as \textit{parent error} (PE). In case of a graph with adjacency matrix $A^i$ in which every node only has parents equal in count to $target\_parents$, $PE(A^i)=0$. For all other graphs, $PE(A^i) > 0$.}

\begin{itemize}
    \item the sum of abstraction estimates included in the candidate graph via arcs
    \item the number of arcs
    \item the number of children per node
    \item the number of parents per node
\end{itemize}

\begin{align*}
    score(D, A) = \\
        \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} (\alpha D_{ij} - \beta A_{ij}) \\
    - \gamma \frac{1}{n} \sum_{i=1}^{n} \left|(\sum_{j=1}^{n}A_{ij}) - children\right| \\
    - \delta \frac{1}{n} \sum_{j=1}^{n} \left|(\sum_{i=1}^{n}A_{ij}) - parents\right|
\end{align*}

Where \textit{A} is the adjacency matrix, \textit{D} is the difference matrix, \textit{n} is the number of dimensions in the difference matrix, and $\alpha, \beta, \gamma, \delta$ are weights for different components of the linear combination. Additionally, \textit{children} and \textit{parents} are the ideal number of children and parents of each node in the graph.

\begin{algorithm}[!tbp] 
    \caption{Graph Search in NSC}
    \label{alg:gs}
    \begin{algorithmic}
        \STATE $s \Leftarrow 0 $ (fully-disconnected graph) \\
        \FOR{$k = 0$ to $epochs$}
        \STATE $T \Leftarrow 1 - \frac{k}{epochs}$ \\
        \STATE $s_{new} \Leftarrow neighbor(s)$ \\
        \IF{$P(score(D, s), score(D, s_{new}), T) \geq random(0, 1)$}
            \STATE $s \Leftarrow s_{new}$
        \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

The result of the graph search is the final output of NSC: a graph which indicates how the underlying ML model relates concepts by means of contextual embeddings.

\begin{algorithm}[!tbp] 
    \caption{Nested State Clouds}
    \label{alg:gs}
    \begin{algorithmic}
        \FOR{$s$ in $symbols$}
            \STATE $a \Leftarrow contexts(s)$ \\
            \STATE $b \Leftarrow cloud(a)$ \\
            \STATE $c \Leftarrow conceptor(b)$ \\
        \ENDFOR
        \FOR{$i, c_i$ in $conceptors$}
            \FOR{$j, c_j$ in $conceptors$}
                \STATE $D_{ij} \Leftarrow f(c_i, c_j)$
            \ENDFOR
        \ENDFOR
        \STATE $s_{output} \Leftarrow graph\_search(D)$
    \end{algorithmic}
\end{algorithm}