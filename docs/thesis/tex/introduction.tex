\section{Introduction}\label{sec:introduction}

In the past years, DL models have been claimed to reach human parity on a range of tasks which were deemed challenging only a few years ago. For instance, machine translation is deemed on par with human translators on popular language pairs \citep{toral_attaining_2018}, scientific DL models are more accurate than explicit hand-crafted ones in a growing range of fields \citep{ravuri_skilful_2021}, while reinforcement learning agents based on DL models have outperformed professionals in multiple video games \citep{schrittwieser_mastering_2020} and industrial control applications \citep{degrave_magnetic_2022}.

The resounding success of recent DL work has in large part been attributed to the newly-gained ability of DL models to automatically extract relevant features from input data \citep{lecun_deep_2015}, rather than making use of hand-crafted features. This has proven an instrumental goal in advancing the state-of-the-art on numerous DL tasks \citep{radford_learning_2021} \citep{radford_improving_nodate}. Concretely, the automatically-derived features are represented through the specific activation patterns of hidden layers as information propagates through the DL model \citep{lecun_deep_2015}. Besides early layers being able to already extract surface input features, a recurring finding has been the fact that input representations become increasingly abstract with each sequential layer, before collapsing again to the concrete particularities of the output towards the final layers \citep{tenney_bert_2019}. When such representations are continuous and dense, they are referred to as embeddings.

In the context of our increased reliance on DL models on a societal level, the field of explainable AI (XAI) investigates methods for interpreting the inner workings of such models, which otherwise lack clear internal rules due to being largely trained on raw data \citep{arrieta_explainable_2019}. Techniques of this kind help researchers debug such DL systems, ensuring their safe use in practice. A pervasive trade-off in XAI, however, is the conflict between formulating explanations which (1) accurately reflect the actual processing performed by DL models (i.e. functionally-grounded), yet (2) are highly comprehensible and intelligible for humans (i.e. human-grounded) \citep{madsen_post-hoc_2021}. This echoes a constant conflict faced in machine translation, where models should (1) adequately preserve input meaning (i.e. adequacy), while (2) producing a coherent translation in itself (i.e. fluency) \citep{koehn_neural_2017}. Framing explainable AI as a translation task from machine to human representations has proven a useful lens for understanding the obstacles faced by the current work.

Given the central role of input representations in recent DL models, a large body of advances in XAI has focused on distilling high-dimensional embeddings into a form which is cognitively ergonomic for humans \citep{madsen_post-hoc_2021}. As the embeddings themselves arguably lack meaning when separated from input or output data, the vast majority of such work has specifically attempted to highlight how a certain DL model relates inputs and outputs by means of embeddings.

For instance, prior work has highlighted toxic gender biases in word embeddings (e.g. "woman" being represented as closer in meaning to "nurse" than "programmer" by an DL model), which promptly led to debiasing techniques being developed by the NLP community \citep{bolukbasi_man_2016}. Moreover, methods have been developed to construct ontologies from embeddings by means of hierarchical clustering, in order to better understand how the underlying DL model groups concepts together \citep{liu_interpretation_2018}. As another line of work, behavioral and structural probes have been employed to locate the "site" of various computations (e.g. part-of-speech tagging in NLP), by means of relating embeddings from different layers to cruder external representations (e.g. part-of-speech) \citep{tenney_bert_2019}. Additionally, methods have been suggested to explicitly represent part-whole relations using embedding "columns", hinting at future DL models which are partially explainable themselves, even before making use of post-hoc XAI tools \citep{hinton_how_2021}.

However, even when interpretability techniques focus on directly relating inputs to outputs, embeddings are often involved as mediators \citep{danilevsky_survey_nodate}. For instance, input feature explanations highlight which particular aspects of the input data have been most influential in yielding the output \citep{jain_attention_nodate}. Alternatively, techniques based on counterfactuals and adversarial examples aim to find marginally different inputs which cause massive changes in output \citep{madsen_post-hoc_2021}. As a particularly ergonomic family of explanations, methods have also been developed to incentivize models to directly explain themselves in natural language \citep{madsen_post-hoc_2021}. Finally, another effective technique is based on extracting knowledge graphs by inferring entity-relationship-entity triples directly via (masked) language modeling \citep{wang_language_2020}.

In this context, we extend the existing toolkit of interpretability techniques with a novel approach termed Nested State Clouds (NSC). This technique can be used to distill highly-comprehensible knowledge graphs directly from sets of high-dimensional embeddings, with no modality constraints. In other words, given a DL model and an auxiliary dataset, NSC appears to be a promising candidate technique for automatically organizing concepts in a part-whole hierarchy which reflects the model's internalized knowledge. However, in the context of this paper, we limit ourselves to an initial investigation of a pretrained masked \textit{language} model. Given this, we leave the applicability of NSC to arbitrary classification models (e.g. ViT \citep{dosovitskiy_image_2021}) for future work. We speculate on the tractability of generalizing NSC in the discussion by hinting at the possibility of making use of state clouds obtained from individual class member embeddings, rather than token embeddings. In light of this, an important benefit of NSC is its modality-agnostic design (i.e. appears applicable to DL models operating with various non-text modalities \citep{noauthor_multimodal_nodate}), in stark contrast to methods which only focus on distilling knowledge graphs from text \citep{wang_language_2020}. By operating with spatial layouts of embeddings, NSC can distill high-dimensional representations which have been abstracted away from particular modalities.

\textbf{Before introducing the core ideas behind NSC, let us specify in more depth several term which will be made heavy use of. By \textit{symbol}, we refer to the explicit way in which a concept is represented in text (e.g. the string "fruit"). Interestingly enough, our discussion illustrates plainly how the same symbol can refer to multiple concepts. By \textit{exemplar}, we refer to a specific instance of a symbol in a certain context (e.g. "It's healthy to eat \textbf{fruit}s regularly."). Importantly, each exemplar is a case of a symbol assuming specific semantics which are unique to its context (e.g. "fruit" could refer to an apple or a banana, depending on context). In practice, those specific semantics are represented numerically by means of contextual embeddings \citep{devlin_bert_nodate}. For the purposes of this work, we generally assume an exemplar-based view of concepts (e.g. defining the concept of fruit by means of the finite totality of its exemplars), as opposed to a prototype-based one (e.g. defining the concept of fruit based on one general idealized prototype).}

NSC works by first generating a state cloud of contextual embeddings for each given symbol, representing the semantics of its exemplars. This generation process is based on simply using the investigated model in inference mode together with the auxiliary corpus. For instance, the symbol "fruit" can refer to a host of different objects, depending on the context of use -- variation which is captured by distinct contextual embeddings. Following this initial step, NSC will have formed a collection of one state cloud of contextual embeddings per symbol. Each such state cloud will represent the distribution of semantics assumed by the various exemplars in their contexts.

Next, NSC compactly represents the overarching shape of each resulting state cloud as a \textit{conceptor} identified with a high-dimensional ellipsoid, instead of a set of contextual embeddings \citep{jaeger_controlling_2017}. We find that this often results in orders-of-magnitude lower memory footprint. While the ellipsoids are conceptually similar to the ones obtained using principal component analysis (PCA), we opted for using conceptors as compact high-dimensional objects due to existing literature investigating meaningful ways of relating them to each other \citep{jaeger_using_nodate}. Specifically, an inherent \textit{abstraction ordering} has been previously defined for pairs of conceptors. This refers to a means of comparing two such objects in terms of their level of abstraction, a process which we are exploiting in the present paper in an attempt to relate concepts in a knowledge graph. Loosely speaking, a conceptor which spatially engulfs another can be said to be more abstract, as the former encompasses a broader region of semantic space than the latter. We refer the reader to the relevant methods subsections for details on conceptors and their abstraction ordering.

After generating one state cloud per symbol, representing them as conceptor objects, and conducting pairwise comparisons of abstraction, an optimization algorithm is employed to generate the final output of NSC: a knowledge graph. Concretely, the algorithm based on simulated annealing is searching for a directed graph which accurately represents the estimated relations of abstraction. In contrast to simply constructing a directed graph by adding a new arc for each positive abstraction relation, a search algorithm allows us to better deal with noise. Additionally, the search framing enables us to specify additional "nice-to-have" properties of the desired output graph \citep{madsen_post-hoc_2021}. For instance, we penalize high numbers of arcs, parents per node, and children per node, in an attempt to keep the output explanations sparse and highly intelligible. In this, the objective function of the search algorithm essentially provides a "slider" between functionally-grounded and human-grounded explanations.

Attempting to place NSC in the existing landscape of interpretability techniques, we note that our approach can generate global explanations (i.e. which attempt to describe the model's processing across inferences) which are provided post-hoc (i.e. after training the model) \citep{danilevsky_survey_nodate}. This is in contrast to those XAI techniques which yield local explanations (i.e. describing the way a particular inference unfolds across layers) and those which are provided during the actual training of inherently interpretable models. Beyond this general placement of NSC in the XAI literature, we point out relevant similarities and differences to prior art throughout the paper.

Our contributions are as follows:

\begin{itemize}
    \item We provide qualitative evidence highlighting the connection between the spatial layout of nested state clouds and the abstraction relation of the concepts they represent.
    \item We formulate an novel algorithm for flexibly distilling a set of high-dimensional state clouds into a compact directed graph which depicts relations of abstraction.
    \item We draw evidence-based observations on the way individual symbols relate to concepts.
\end{itemize}