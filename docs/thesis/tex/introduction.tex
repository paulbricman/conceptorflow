\section{Introduction}\label{sec:introduction}

In the past years, ML models have been claimed to reach human parity on a range of tasks which were deemed challenging only years prior. For instance, machine translation is deemed on par with human translators on popular language pairs, scientific ML models are more accurate than explicit hand-crafted ones in a growing range of fields, while RL agents have outperformed professionals in multiple video games and industrial control applications.

The resounding success of recent ML work has in large part been attributed to the newly-gained ability of ML models to automatically extract relevant features from input data, rather than making use of hand-crafted features. This has proven an instrumental goal in advancing the state-of-the-art on the vast majority of ML tasks. Concretely, the automatically-derived features are represented through the specific activation patterns of hidden layers as information propagates through the ML model. Besides early layers being able to already extract surface input features, a recurring finding has been the fact that input representations become increasingly abstract with each sequential layer, before collapsing again to the concrete particularities of the output towards the final layers. When such representations are continuous and dense, they are referred to as embeddings.

In the context of our increased reliance on ML models on a societal level, the field of explainable AI (XAI) investigates methods for interpreting the inner workings of such models, which otherwise lack clear internal rules due to being largely trained on raw data. Techniques of this kind help researchers debug such ML systems, ensuring their safe use in practice. A pervasive trade-off in XAI, however, is the conflict between formulating explanations which (1) accurately reflect the actual processing performed by ML models (i.e. functionally-grounded), yet (2) are highly comprehensible and intelligible for humans (i.e. human-grounded). This echoes a constant conflict faced in machine translation, where models should (1) adequately preserve input meaning (i.e. adequacy), while (2) producing a coherent translation in itself (i.e. fluency). Framing explainable AI as a translation task from machine to human representations has proven a useful lens for understanding the obstacles faced by the current work.

Given the central role of input representations in recent ML models, a large body of advances in XAI has focused on distilling high-dimensional embeddings into a form which is cognitively ergonomic for humans. As the embeddings themselves arguably lack meaning when separated from input or output data, the vast majority of such work has specifically attempted to highlight how a certain ML model relates inputs and outputs by means of embeddings.

For instance, prior work has highlighted toxic gender biases in word embeddings (e.g. "woman" being represented as closer in meaning to "nurse" than "programmer" by an ML model), which promptly led to debiasing techniques being developed by the NLP community. Moreover, methods have been developed to construct ontologies from embeddings by means of hierarchical clustering, in order to better understand how the underlying ML model groups concepts together. As another line of work, behavioral and structural probes have been employed to locate the "site" of various computations (e.g. part-of-speech tagging in NLP), by means of relating embeddings from different layers to cruder external representations (e.g. part-of-speech). Additionally, methods have been suggested to explicitly represent part-whole relations using embedding "columns", hinting at future ML models which are partially explainable themselves, even before making use of post-hoc XAI tools.

However, even when interpretability techniques focus on directly relating inputs to outputs, embeddings are often involved as mediators. For instance, input feature explanations highlight which particular aspects of the input data have been most influential in yielding the output. Alternatively, techniques based on counterfactuals and adversarial examples -- often confounded in the literature -- aim to find marginally different inputs which cause massive changes in output. As a particularly ergonomic family of explanations, methods have also been developed to incentivize models to directly explain themselves in natural language. Finally, another effective technique is based on extracting knowledge graphs by inferring entity-relationship-entity triples directly via (masked) language modeling.

In this context, we extend the existing toolkit of interpretability techniques with a novel approach called Nested State Clouds (NSC). This technique can be used to distill highly-comprehensible knowledge graphs directly from sets of high-dimensional embeddings, with no modality constraints. In other words, given an ML model and an auxiliary dataset, NSC can be used to automatically organize concepts in a part-whole hierarchy which, in large part, reflects the model's internalized knowledge. As investigated in this paper, this novel interpretability technique can be used to surface learned ontologies from sequence-to-sequence models (e.g. BERT, GPT). However, NSC can also be applied to arbitrary classification models (e.g. ViT), by creating state clouds from individual class member embeddings, as mentioned in the discussion. Given this, an important benefit of NSC is its modality-agnostic nature, in stark contrast to methods which only focus on distilling knowledge graphs from text. By operating with spatial layouts of embeddings, NSC can distill high-dimensional representations which have been abstracted away from particular modalities.

NSC works by first generating a state cloud of contextual embeddings for each given entity, representing the different meanings of the entity in different contexts. For instance, the concept "fruit" can refer to a host of different objects, depending on the context of use -- variation which is captured by distinct contextual embeddings. Next, NSC compactly represents the overarching shape of the resulting state cloud as a high-dimensional ellipsoid, instead of a set of embeddings, which often results in orders-of-magnitude lower memory footprint. While the ellipsoids resemble PCA and SVD outputs, we opted for using conceptors as compact high-dimensional objects due to existing literature investigating meaningful ways of relating them to each other. Specifically, it has been posited -- yet never before tested empirically, to the best of our knowledge -- that conceptors benefit from an inherent abstraction ordering, a means of comparing two such objects in terms of their level of abstraction. Loosely speaking, a conceptor which spatially engulfs another can be said to be more abstract, as it encompasses a broader region of space than the other one.

After generating state clouds, representing them as conceptor objects, and conducting pairwise comparisons of abstraction, a search algorithm is employed to find a directed graph which accurately represents the estimated relations of abstraction. In contrast to simply constructing a directed graph by adding a new arc for each positive abstraction relation, a search algorithm allows us to better deal with noise. Additionally, the search framing enables us to specify additional "nice-to-have" properties of the desired output graph. For instance, we penalize high numbers of arcs, parents per node, and children per node, in an attempt to keep the output explanations sparse and highly intelligible. In this, the objective function of the search algorithm essentially provides a "slider" between functionally-grounded and human-grounded explanations.

Attempting to place NSC in the existing landscape of interpretability techniques, we note that our approach can generate global explanations (i.e. holistically describing the model's processing across inferences) which are provided post-hoc (i.e. after training the model). This is in contrast to those XAI techniques which yield local explanations (i.e. describing the way a particular inference unfolds across layers) and those which are provided during the actual training of inherently interpretable models. Beyond this general placement of NSC in the XAI literature, we point out relevant similarities and differences to prior art throughout the paper.

Our contributions are as follows:

\begin{itemize}
    \item We provide qualitative evidence highlighting the connection between the spatial layout of nested state clouds and the abstraction relation of the concepts they represent;
    \item We formulate an novel algorithm for flexibly distilling a set of high-dimensional state clouds into a compact directed graph which depicts part-whole relations;
    \item We draw evidence-based observations on the way individual symbols relate to concepts.
\end{itemize}