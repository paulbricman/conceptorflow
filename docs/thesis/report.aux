\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{toral_attaining_2018}
\citation{ravuri_skilful_2021}
\citation{schrittwieser_mastering_2020}
\citation{degrave_magnetic_2022}
\citation{lecun_deep_2015}
\citation{radford_learning_2021}
\citation{radford_improving_nodate}
\citation{lecun_deep_2015}
\citation{tenney_bert_2019}
\citation{arrieta_explainable_2019}
\citation{madsen_post-hoc_2021}
\citation{koehn_neural_2017}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{1}{subsection.1.1}\protected@file@percent }
\citation{madsen_post-hoc_2021}
\citation{bolukbasi_man_2016}
\citation{liu_interpretation_2018}
\citation{tenney_bert_2019}
\citation{hinton_how_2021}
\citation{danilevsky_survey_nodate}
\citation{jain_attention_nodate}
\citation{madsen_post-hoc_2021}
\citation{madsen_post-hoc_2021}
\citation{wang_language_2020}
\citation{wang_language_2020}
\citation{dosovitskiy_image_2021}
\citation{danilevsky_survey_nodate}
\citation{devlin_bert_nodate}
\citation{jaeger_controlling_2017}
\citation{jaeger_controlling_2017}
\citation{madsen_post-hoc_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}NSC Overview}{3}{subsection.1.2}\protected@file@percent }
\citation{devlin_bert_nodate}
\citation{reimers_sentence-bert_2019}
\citation{yin_benchmarking_2019}
\citation{jiang_evaluating_2019}
\citation{bahdanau_neural_2016}
\citation{tenney_bert_2019}
\citation{liu_roberta_2019}
\citation{lan_albert_2020}
\citation{sanh_distilbert_2020}
\citation{he_deberta_2021}
\citation{devlin_bert_nodate}
\citation{Zhu2015AligningBA}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{4}{section.2}\protected@file@percent }
\newlabel{sec:methods}{{2}{4}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Data}{4}{subsection.2.2}\protected@file@percent }
\citation{jaeger_controlling_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Conceptors}{5}{subsection.2.3}\protected@file@percent }
\citation{jaeger_controlling_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A: Exemplars are located and extracted together with their surrounding contexts. B: Each exemplar is encoded into a contextual embedding. Notably, the same symbol is encoded into different such embeddings based on context, resulting in a state cloud per symbol. C: Conceptors are derived from each state cloud as more compact representations.\relax }}{6}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:exemplars-conceptors}{{2.1}{6}{A: Exemplars are located and extracted together with their surrounding contexts. B: Each exemplar is encoded into a contextual embedding. Notably, the same symbol is encoded into different such embeddings based on context, resulting in a state cloud per symbol. C: Conceptors are derived from each state cloud as more compact representations.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Abstraction Ordering}{6}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A: The matrix $D$ represents the pairwise relations of abstraction between conceptors. B: Matrix $D$ is populated using abstraction heuristics, where $D_{ij}$ approximates how much more abstract conceptor $C_i$ is with respect to conceptor $C_j$. C: The graph optimization process attempts to generate an ontology which is (1) faithful to the relations of abstraction identified previously, and (2) legible and sparse.\relax }}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:conceptors-graph}{{2.2}{7}{A: The matrix $D$ represents the pairwise relations of abstraction between conceptors. B: Matrix $D$ is populated using abstraction heuristics, where $D_{ij}$ approximates how much more abstract conceptor $C_i$ is with respect to conceptor $C_j$. C: The graph optimization process attempts to generate an ontology which is (1) faithful to the relations of abstraction identified previously, and (2) legible and sparse.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Graph Optimization}{7}{subsection.2.5}\protected@file@percent }
\citation{madsen_post-hoc_2021}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces Graph optimization in NSC\relax }}{9}{algorithm.2.1}\protected@file@percent }
\newlabel{alg:gs}{{2.1}{9}{Graph optimization in NSC\relax }{algorithm.2.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.2}{\ignorespaces Nested State Clouds\relax }}{9}{algorithm.2.2}\protected@file@percent }
\newlabel{alg:nsc}{{2.2}{9}{Nested State Clouds\relax }{algorithm.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{10}{section.3}\protected@file@percent }
\newlabel{sec:results}{{3}{10}{Results}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces NSC output graph when applied to BERT using the shown symbols.\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:nsc_output_graph}{{3.1}{10}{NSC output graph when applied to BERT using the shown symbols.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Candidate score by epoch during the graph optimization process.\relax }}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:nsc_score_history}{{3.2}{10}{Candidate score by epoch during the graph optimization process.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces NSC output graph when applied to BERT using the shown symbols.\relax }}{11}{figure.caption.5}\protected@file@percent }
\newlabel{fig:valuable}{{3.3}{11}{NSC output graph when applied to BERT using the shown symbols.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces NSC output graph when applied to BERT using the shown symbols.\relax }}{11}{figure.caption.6}\protected@file@percent }
\newlabel{fig:spectrum}{{3.4}{11}{NSC output graph when applied to BERT using the shown symbols.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces NSC output graph after undervaluing children count per node constraints.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:children}{{3.5}{11}{NSC output graph after undervaluing children count per node constraints.\relax }{figure.caption.7}{}}
\citation{madsen_post-hoc_2021}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces NSC output graph after undervaluing arc pruning.\relax }}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:pruning}{{3.6}{12}{NSC output graph after undervaluing arc pruning.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{12}{section.4}\protected@file@percent }
\newlabel{sec:discussion}{{4}{12}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Potential issues}{12}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}The same symbols can represent different concepts.}{12}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces 2D PCA projection of the state cloud associated with the symbol "plant"\relax }}{12}{figure.caption.9}\protected@file@percent }
\newlabel{fig:plant}{{4.1}{12}{2D PCA projection of the state cloud associated with the symbol "plant"\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Context samples by K-means cluster of "plant" state cloud.\relax }}{12}{table.caption.10}\protected@file@percent }
\newlabel{tab:samples}{{4.1}{12}{Context samples by K-means cluster of "plant" state cloud.\relax }{table.caption.10}{}}
\citation{west_symbolic_2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}State clouds are non-linear.}{13}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces 2D PCA projection of the state cloud associated with the symbol "earth"\relax }}{13}{figure.caption.11}\protected@file@percent }
\newlabel{fig:earth}{{4.2}{13}{2D PCA projection of the state cloud associated with the symbol "earth"\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}NSC requires many exemplars.}{13}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}NSC is influenced by the choice of auxiliary data.}{13}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Future work}{14}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Improved noise robustness}{14}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Improved graph optimization scalability}{14}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Beyond abstraction ordering}{14}{subsubsection.4.2.3}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{literature}
\bibcite{arrieta_explainable_2019}{{1}{2019}{{Arrieta et~al.}}{{Arrieta, Díaz-Rodríguez, Del~Ser, Bennetot, Tabik, Barbado, García, Gil-López, Molina, Benjamins, Chatila, and Herrera}}}
\bibcite{bahdanau_neural_2016}{{2}{2015}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{bolukbasi_man_2016}{{3}{2016}{{Bolukbasi et~al.}}{{Bolukbasi, Chang, Zou, Saligrama, and Kalai}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Beyond token embeddings}{15}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Alternative abstraction heuristics}{15}{subsubsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Conclusion}{15}{subsection.4.3}\protected@file@percent }
\bibcite{danilevsky_survey_nodate}{{4}{2020}{{Danilevsky et~al.}}{{Danilevsky, Qian, Aharonov, Katsis, Kawas, and Sen}}}
\bibcite{degrave_magnetic_2022}{{5}{2022}{{Degrave et~al.}}{{Degrave, Felici, Buchli, Neunert, Tracey, Carpanese, Ewalds, Hafner, Abdolmaleki, de~las Casas, Donner, Fritz, Galperti, Huber, Keeling, Tsimpoukelli, Kay, Merle, Moret, Noury, Pesamosca, Pfau, Sauter, Sommariva, Coda, Duval, Fasoli, Kohli, Kavukcuoglu, Hassabis, and Riedmiller}}}
\bibcite{devlin_bert_nodate}{{6}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dosovitskiy_image_2021}{{7}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{he_deberta_2021}{{8}{2021}{{He et~al.}}{{He, Liu, Gao, and Chen}}}
\bibcite{hinton_how_2021}{{9}{2021}{{Hinton}}{{}}}
\bibcite{jaeger_controlling_2017}{{10}{2017}{{Jaeger}}{{}}}
\bibcite{jain_attention_nodate}{{11}{2019}{{Jain and Wallace}}{{}}}
\bibcite{jiang_evaluating_2019}{{12}{2019}{{Jiang and de~Marneffe}}{{}}}
\bibcite{koehn_neural_2017}{{13}{2017}{{Koehn}}{{}}}
\bibcite{lan_albert_2020}{{14}{2020}{{Lan et~al.}}{{Lan, Chen, Goodman, Gimpel, Sharma, and Soricut}}}
\bibcite{lecun_deep_2015}{{15}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{liu_interpretation_2018}{{16}{2018}{{Liu et~al.}}{{Liu, Huang, Li, and Hu}}}
\bibcite{liu_roberta_2019}{{17}{2019}{{Liu et~al.}}{{Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}}}
\bibcite{madsen_post-hoc_2021}{{18}{2021}{{Madsen et~al.}}{{Madsen, Reddy, and Chandar}}}
\bibcite{radford_improving_nodate}{{19}{2018}{{Radford et~al.}}{{Radford, Narasimhan, Salimans, and Sutskever}}}
\bibcite{radford_learning_2021}{{20}{2021}{{Radford et~al.}}{{Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever}}}
\bibcite{ravuri_skilful_2021}{{21}{2021}{{Ravuri et~al.}}{{Ravuri, Lenc, Willson, Kangin, Lam, Mirowski, Fitzsimons, Athanassiadou, Kashem, Madge, Prudden, Mandhane, Clark, Brock, Simonyan, Hadsell, Robinson, Clancy, Arribas, and Mohamed}}}
\bibcite{reimers_sentence-bert_2019}{{22}{2019}{{Reimers and Gurevych}}{{}}}
\bibcite{sanh_distilbert_2020}{{23}{2020}{{Sanh et~al.}}{{Sanh, Debut, Chaumond, and Wolf}}}
\bibcite{schrittwieser_mastering_2020}{{24}{2020}{{Schrittwieser et~al.}}{{Schrittwieser, Antonoglou, Hubert, Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and Silver}}}
\bibcite{tenney_bert_2019}{{25}{2019}{{Tenney et~al.}}{{Tenney, Das, and Pavlick}}}
\bibcite{toral_attaining_2018}{{26}{2018}{{Toral et~al.}}{{Toral, Castilho, Hu, and Way}}}
\bibcite{wang_language_2020}{{27}{2020}{{Wang et~al.}}{{Wang, Liu, and Song}}}
\bibcite{west_symbolic_2021}{{28}{2021}{{West et~al.}}{{West, Bhagavatula, Hessel, Hwang, Jiang, Bras, Lu, Welleck, and Choi}}}
\bibcite{yin_benchmarking_2019}{{29}{2019}{{Yin et~al.}}{{Yin, Hay, and Roth}}}
\bibcite{Zhu2015AligningBA}{{30}{2015}{{Zhu et~al.}}{{Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba, and Fidler}}}
