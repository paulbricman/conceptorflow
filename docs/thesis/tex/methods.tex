\section{Methods}\label{sec:methods}

\subsection{Model}

\textbf{As the object of our interpretation technique, we chose a pretrained BERT model, short for Bidirectional Encoder Representations from Transformers. For completeness, BERT is a transformer model which maps a set of subword tokens to another set of such tokens. BERT has been originally trained on two different natural language processing objectives. First, it has been tasked with a masked language modeling (DLM) objective. This refers to the task of reconstructing a short input text which has been intentionally corrupted. The corruption typically consists in eliminating (i.e. masking) a random proportion of the tokens contained in the input text (e.g. "BERT is a transformer model." might be corrupted as "BERT is a [MASK] model."). Given this, the DLM task consists in reconstructing the pre-corruption text from the corrupted version.}

\textbf{The second objective employed in training BERT is a next sentence prediction (NSP) task. Given a pair of two sentences, BERT is tasked with predicting whether they are consecutive in the original text. The combination of those two conceptually simple objectives has been shown to help BERT learn rich semantic representations of the text being processed, as an instrumental goal in solving the two tasks. For instance, mean-pooling token embeddings across texts has been shown to be highly effective in downstream information retrieval tasks based on vector similarity. Moreover, mean-pooling token embeddings of a text and comparing the result with the mean-pooled embeddings of a set of labels (e.g. science, politics, economics), has been shown to be a competitive baseline in text classification. Alternatively, BERT models fine-tuned on limited data from other tasks (e.g. natural language inference) had yielded state-of-the-art performance in multiple tasks.}

\textbf{Internally, BERT represents each token as an embedding of dimensionality 768. As the model consists of a repetitive sequence of layers, the set of embeddings which represents tokens is adjusted from one layer to the next using multi-head attention mechanisms. It is precisely those token embeddings which we are trying to distill knowledge graphs from using the NSC approach. Specifically, as often done in prior art, we are attempting to interpret the set of embeddings which are generated in the \textit{last} BERT layer. This has been hypothesized to contain high-level features extracted the input tokens which are based in large part on the tokens' contexts, after extensive processing in the earlier layers of the model.}

We opted for BERT due to its widespread use in industry applications and its large number of derivative models (e.g. RoBERTa, ALBERT, distilBERT, etc.). BERT takes in a sequence of subword tokens as input and reconstructs it as output, while generating a unique contextual embedding for each token. Crucially, the same token can be attributed different embeddings in different contexts (e.g. "she" referring to different people). In practice, the contextual embeddings of individual tokens are mean-pooled together to yield an overarching document embedding. However, here we focus only on the contextual embeddings of individual tokens or at most short sequences of them which form a noun phrase (e.g. "orange juice").

\subsection{Data}

As NSC requires an auxiliary dataset for generating state clouds of contextual embeddings, we employ one of the datasets which have been used for training the BERT model, namely BookCorpus. This dataset consists of a variety of public domain books across different genres, and provides many different contexts for tokens to appear in.

We focus our investigation on relating a set of 100 hand-picked concepts to each other. For each concept, we extract all contexts in which they appear verbatim in the dataset. A context is defined as the span of text starting 300 characters before and ending 300 characters after the concept occurence. Additionally, we trim the incomplete beginning and ending sentences (i.e. trailing) from each context, leaving in only complete sentences surrounding the concept occurence.

For each context, we extract the contextual embedding of the concept occurence, obtaining a set of such embeddings for each concept. The cardinality of each set depends on the frequency of occurence of the concept in the dataset. We further filter our set of concepts based on the size of the set of contextual embeddings, eliminating concepts which had fewer occurences than the number of BERT embedding dimensions (i.e. 768). We cover difficulties in handling sparser state clouds in the discussion.

\subsection{Conceptors}

\textbf{From each remaining state cloud representing the set of concept nuances used in the dataset, we learn a conceptor. For completeness, a conceptor is a mathematical object which models the distribution of state cloud in its space. However, conceptors do \textit{not} represent the \textit{density} of embeddings in space using a probability density function. Rather, a conceptor represents the \textit{dimensions} across which the state cloud spreads most across space, together with the spread associated with each dimension. This information can be compactly represented in a square matrix whose dimensionality matches the one of the space populated by the state cloud.}

\textbf{Both conceptors and Principal Component Analysis (PCA) make use of the correlation matrix of the state cloud. However, an additional parameter appears in the case of conceptors. Specifically, a conceptor also requires an aperture to be defined. The aperture $\alpha$ is a parameter which dictates the extent to which the state cloud is reflected in the associated conceptor object. For increasing aperture values, the conceptor matrix approaches the identity matrix. For decreasing aperture values, the conceptor matrix approaches the zero matrix.}

\textbf{Obtaining a conceptor from a state cloud is straight-forward and computationally cheap. Given the correlation matrix of the state cloud and a real value specified for the aperture parameter, the conceptor matrix can be obtained through the following closed-form equation:$$C(R, \alpha) = R (R + \alpha^{-2} I)^{-1}.$$}

\textbf{In the present work, we only obtain conceptors from state clouds composed of contextual embeddings generated by the pretrained BERT model. Each state cloud contains contextual embeddings associated with one symbol composed of one or several tokens (e.g. "orange juice"), while each individual contextual embedding is associated with a specific occurence of the symbol in the text corpus. Given that we obtain conceptors from state clouds of BERT contextual embeddings, the dimensionality of the state cloud will match that of the embeddings, namely $$d_{BERT}=768.$$ We note that the interpretability technique we introduce does not require this specific dimensionality. Rather, the specific value of $d_{BERT}$ is only an artifact of the investigated model's own architecture. It is conceivable that NSC could be applied with minimal modifications to state clouds of lower or higher dimensionality. In fact, some of the experiments used to introduce conceptors as a mathematical object make use of state clouds of only several dimensions.}

\textbf{Additionally, it is useful to note the large difference in terms of memory footprint observed between a state cloud of BERT embeddings and a conceptor matrix derived from it. A state cloud containing $n_embs = 10^6$ BERT embeddings of dimensionality $d_{BERT}=768$ naively requires $n_{embs} \cdot d_{BERT} = 7.68 * 10^8$ floating point values to fully represent. In contrast, the conceptor matrix obtained from the same state cloud, given a certain aperture, is a square matrix with $d_{BERT}$ rows and columns. Hence, it only needs $d_{BERT}^2 \approx 5.89 * 10^5$ floating point values to be represented. In this specific case, the conceptor represents the state cloud with an approximately $10^3$ times smaller memory footprint. Moreover, the memory footprint of the conceptor matrix is constant with respect to the cardinality of the state cloud. It is only the accuracy of representing the state cloud which increases with more samples in the form of new contextual embeddings, as the correlation matrix converges.The implication of this is that state clouds associated with symbols which have relatively high frequency in the auxiliary dataset get represented through conceptor matrices of the same size as those associated with symbols with relatively low frequency.}

\subsection{Abstraction Ordering}

\textbf{For each pair of conceptors $(C_1, C_2)$ obtained from different state clouds of contextual embeddings, we attempt to estimate how they relate to each other in terms of abstraction. We aim for determining whether (1) $C_1$ represents a concept which is more abstract than the one represented by $C_2$ (e.g. "fruit" $>$ "apple"), whether (2) it is the other way around (e.g. "apple" $<$ "fruit"), or whether (3) the two concepts represented lack a meaningful abstraction ordering (e.g. "fruit" $<>$ "galaxy"). Ideally, we would only want the final knowledge graph generated by NSC to represent relations of decreasing abstraction by means of arcs (e.g. "fruit" $>$ "apple"). This would lead to a readable knowledge graph which approaches a hierarchical structure.}

\textbf{In its original formulation, abstraction ordering of conceptors has two important characteristics. First, prior art only describes the three mutually-exclusive cases above, with hard limits. $C_1$ is described to be more abstract than $C_2$ if and only if the difference matrix $C_1 - C_2$ is positive definite. Due to the symmetry of abstraction ordering, $C_2 > C_1$ if and only if the difference matrix $C_2 - C_1$ is positive definite. Unfortunately, real data is noisy, making it extremely unlikely that the unambigious criterion of positive definiteness ever holds for conceptors obtained from non-synthetic data (e.g. BERT contextual embeddings). Besides the inevitable aleatoric noise associated with non-synthetic data, abstraction ordering in its original formulation is also hindered by the cumulative error introduced by limited machine precision when dealing with floating point values. Approaches from numerical methods, however, might help mitigate the impact of this second source of noise.}

\textbf{Besides requiring standards of precision and signal-to-noise ratio which are non-trivial to attain in practice, abstraction ordering in its original formulation also happens to provide hard cut-offs. $C_1$ can be determined to be more abstract than $C_2$, less abstract, or equally abstract. In practice, a continuous signal representing \textit{how much} more abstract $C_1$ is compared to $C_2$ appears to be quite useful relative to the original ternary signal. In attempting to make use of an abstraction ordering signal which (1) is decently robust against the noise of real-world data, and (2) can be used to gauge the precise \textit{magnitude} of the abstraction difference, we introduce a heuristic. The design of this heuristic has been informed by the fact that symmetric positive definite matrices only have positive eigenvalues. This property has led to the idea of mean-pooling eigenvalues and using both the polarity and magnitude of the result as a proxy for abstraction ordering of two given conceptors.}

\textbf{Concretely, we estimate the abstraction ordering of $(C_1, C_2)$ by means of the following heuristic:$$f(C_1, C_2) = \frac{1}{d_{BERT}} \sum\limits_{i=1}^{d_{BERT}} \lambda_i(C_1 - C_2).$$ To unpack, we first substract one conceptor matrix from the other. Second, we compute the mean of the eigenvalues the difference matrix. Intuitively, all eigenvalues of the difference matrix are positive if the first conceptor spatially engulfs the other, having higher spread than the second across all dimensions. In the context of this project, across all $d_{BERT} = 768$ dimensions. Conversely, all such eigenvalues are negative if the first conceptor is completely contained by the second across all dimensions. Inevitably, however, the two conceptors will exhibit one such relation across \textit{some} dimensions, while simultaneously exhibiting the opposite in other dimensions. Hence, we average the eigenvalues in an attempt to reach a "consensus opinion" as to how the two conceptors are related to each other.}

\subsection{Graph Optimization}

Given the pairwise estimates of abstraction ordering computed before, we conduct a graph optimization process. All candidate graphs considered are directed ones, while nodes are identified with concepts, and arcs indicate meronymous relations of abstraction (i.e. IS\_A).

We attempt to solve the graph optimization task through the local search algorithm of simulated annealing (see \ref{alg:gs}). \textbf{Each candidate graph is represented through a Boolean adjacency matrix $A$. Specifically, $A^k$ denotes the adjacency matrix of the candidate graph considered at step $k$ of the graph optimization process. $A^k_{ij}$ is a Boolean value indicating whether concept $i$ links to concept $j$ in the associated candidate graph of step $k$.} As an initial candidate graph, the optimization process starts with a fully-disconnected graph, where no concepts are related to each other. This is represented through an adjacency matrix full of null values, $A^0 = 0$. Then, we randoDLy sample a new graph proposal by randoDLy mutating the current graph -- removing a previous arc or adding a new one. The acceptance probability is informed by a temperature schedule which linearly decreases from one to zero over the course of the search process, encouraging heavy exploration in the first epochs while using an increasingly conservative strategy towards the end.

\textbf{The objective function which the search algorithm attempts to maximize is a linear combination of four different terms. Each term is a function of either or both (1) the adjacency matrix $A^i$ which is identified with the state of the graph optimization process in step $i$, and (2) the matrix $D$ containing pairwise estimates of abstract ordering. $D_{ij}$ denotes the numerical estimate of abstraction between conceptor $i$ and $j$. In other words, $$D_{ij} = f(C_i, C_j)$$. We note that the particular way $D_{ij}$ is related to $D_{ji}$ is determined by the choice numerical heuristic employed for abstraction ordering. In our case (i.e. mean of eigenvalues of difference matrix), $D_{ij} = -D_{ji}$, yet this is not necessarily the case when opting for other heuristics, as explored in the discussion (e.g. positive-negative eigenvalues ratio). Besides the two matrices just described which influence the objective function through the four terms whose description follows, the objective function is also influenced by the four coefficients which are used to weigh the four terms.}

\textbf{The first term is a function of both $A^i$ and $D$. It is equal to the mean of the abstraction ordering estimates represented in the candidate graph by means of arcs. From now on, we refer to this term as \textit{expressed abstraction} (EA). In case of a fully-disconnected graph (i.e. one in which no arc exists in the graph at all) represented by $A^i = 0$, $EA(A^i, D) = 0$. In contrast, in case of a fully-connected graph (i.e. one in which there exists an arc between any two nodes) represented by $A^i = 1$, $$EA(A^i, D) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij} D_{ij}.$$ In general, $min(D) \leq EA(A^i, D) \leq max(D)$. This is the only among the four terms of the linear combination which indicates functional-groundedness, as it reflects the proportion of the abstraction identified in high-dimensional space which gets represented in the output knowledge graph.}

\textbf{The second term is only a function of $A^i$. It is equal to the proportion of arcs contained by the candidate graph represented by adjacency matrix $A^i$, relative to the maximum number of possible arcs $n^2$: $$AD(A^i) = \frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij}.$$ We refer to this term as \textit{arc density} (AD). In case of a fully-disconnected graph with $A^i = 0$, $AD(A^i) = 0$. In case of a fully-connected graph with $A^i = 1$, $AD(A^i) = 1$. In all other cases, $0 < AD(A^i) < 1$.}

\textbf{The third term is also only a function of $A^i$. It is equal to the mean difference between a node's children count (i.e. number of nodes connected via outbound arcs) and a target children count set in advance: $$CE(A^i) = \frac{1}{n} \sum_{i=1}^{n} \left|(\sum_{j=1}^{n}A_{ij}) - target\_children\right|.$$ We refer to this term as \textit{children error} (CE). In case of a graph with adjacency matrix $A^i$ in which every node only has children equal in count to $target\_children$, $CE(A^i)=0$. For all other graphs, $CE(A^i) > 0$.}

\textbf{The fourth and final term of the objective function is extremely similar to the previous one, yet it addresses the distribution of parent counts, rather than children counts. Concretely, it is equal to the mean difference between a node's parent count (i.e. number of nodes connected via inbound arcs) and a target parent count set in advance: $$PE(A^i) = \frac{1}{n} \sum_{j=1}^{n} \left|(\sum_{i=1}^{n}A_{ij}) - target\_parents\right|.$$ We refer to this term as \textit{parent error} (PE). In case of a graph with adjacency matrix $A^i$ in which every node only has parents equal in count to $target\_parents$, $PE(A^i)=0$. For all other graphs, $PE(A^i) > 0$.}

\textbf{Besides the four terms which are functions of either or both $A^i$ and $D$, the objective function also contain four coefficients meant to influence the relative weight of each term. We denote these as $\alpha, \beta, \gamma$, and $\delta$ in order for the four terms. Given the four terms, the two additional targets for child and parent count, and the four weighing coefficients included in the linear combination, the objective function can finally be defined as:}

\begin{align*}
    & score(A^i, D, target\_children, target\_parents) = \\
    & \alpha EA(A^i, D) \\
    & - \beta AD(A^i) \\
    & - \gamma CE(A^i, target\_children) \\
    & - \delta PE(A^i, target\_parents).
\end{align*}

\begin{algorithm}[!tbp] 
    \caption{Graph Search in NSC}
    \label{alg:gs}
    \begin{algorithmic}
        \STATE $s \Leftarrow 0 $ (fully-disconnected graph) \\
        \FOR{$k = 0$ to $epochs$}
        \STATE $T \Leftarrow 1 - \frac{k}{epochs}$ \\
        \STATE $s_{new} \Leftarrow neighbor(s)$ \\
        \IF{$P(score(D, s), score(D, s_{new}), T) \geq random(0, 1)$}
            \STATE $s \Leftarrow s_{new}$
        \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

The result of the graph search is the final output of NSC: a graph which indicates how the underlying DL model relates concepts by means of contextual embeddings.

\begin{algorithm}[!tbp] 
    \caption{Nested State Clouds}
    \label{alg:gs}
    \begin{algorithmic}
        \FOR{$s$ in $symbols$}
            \STATE $a \Leftarrow contexts(s)$ \\
            \STATE $b \Leftarrow cloud(a)$ \\
            \STATE $c \Leftarrow conceptor(b)$ \\
        \ENDFOR
        \FOR{$i, c_i$ in $conceptors$}
            \FOR{$j, c_j$ in $conceptors$}
                \STATE $D_{ij} \Leftarrow f(c_i, c_j)$
            \ENDFOR
        \ENDFOR
        \STATE $s_{output} \Leftarrow graph\_search(D)$
    \end{algorithmic}
\end{algorithm}

\textbf{In the context of the present work, we have manually specified values for the four coefficients of the linear combination which comprises the graph optimization objective, $\alpha, \beta, \gamma$, and $\delta$. Besides those, we have also manually specified values for the two hyperparameters related to local graph structure, $target\_children$ and $target\_parents$. However, a more robust approach to specifying the values of those six hyperparameters would be to conduct a hyperparameter search. Concretely, one might resort to searching for appropriate values for those six hyperparameters -- in addition to the number of graph optimization epochs, the temperature schedule, and the conceptor aperture -- which successfully recover some presupposed relations of abstraction between the concepts being related by means of the resulting knowledge graph. We leave that for future work and expand on the possibility in the discussion.}