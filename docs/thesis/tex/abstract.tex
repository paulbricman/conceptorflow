\begin{abstract}
{Interpretability techniques help ensure the safe deployment of ML models into production by providing practitioners with diverse debugging tools, yet the inner workings of large models remain elusive. In this work, we propose a novel interpretability technique which can be used to distill sparse knowledge graphs from a model's high-dimensional embeddings. This technique, termed Nested  State Clouds (NSC), takes advantage of the relative spatial layouts of state clouds in latent space (e.g. "fruit" contextual embeddings appear to engulf "apple" ones). We successfully apply NSC to BERT, and recover an ontology of concepts grounded in the model's latent space.\vspace{6ex}}
\end{abstract}
